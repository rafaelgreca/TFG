{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d406ee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unidecode\n",
    "!pip install nltk\n",
    "!pip install tensorflow\n",
    "!pip install sklearn\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2250095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 16:30:24.193855: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-15 16:30:24.193891: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package stopwords to /home/greca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle\n",
    "import gensim\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "os.environ['PYTHONHASHSEED']=str(2109)\n",
    "tf.random.set_seed(2109)\n",
    "random.seed(2109)\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(2109)\n",
    "sns.set_style('dark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20843ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db216db",
   "metadata": {},
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e4e4f",
   "metadata": {},
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open('glove.twitter.27B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = value = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' %len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "029499f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6bfaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45890"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(data['tweet'])\n",
    "data['tokenized'] = tokenizer.texts_to_sequences(data['tweet'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed8e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        embedding_matrix[i] = np.random.randn(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "065a70a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45890"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f078a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"word2vec.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(embedding_matrix, output_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a37073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c1ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_username(text):\n",
    "  text = re.sub(r'\\@[^\\s]+', ' ', text)\n",
    "  return text\n",
    "\n",
    "def remove_newline(text):\n",
    "  text = text.replace('\\n', ' ')\n",
    "  return text\n",
    "\n",
    "def only_letters(text):\n",
    "  text = re.sub(r'[^a-záâàãéêèẽíìîĩóòõôúùũû\\s]+', ' ', text)\n",
    "  return text\n",
    "\n",
    "def remove_link(text):\n",
    "  text = re.sub(r'www\\.?[^\\s]+', ' ', text)\n",
    "  return text\n",
    "\n",
    "def remove_hyperlink(text):\n",
    "  text = re.sub(r'\\<.?\\>', ' ', text)\n",
    "  return text\n",
    "\n",
    "def remove_accent(text):\n",
    "  text = unidecode.unidecode(text)\n",
    "  return text\n",
    "\n",
    "def adjustment_text(text):\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  text = text.strip()\n",
    "  return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  text = [word for word in text.split() if word not in sw]\n",
    "  text = ' '.join(text)\n",
    "  return text\n",
    "\n",
    "def remove_spam(text):\n",
    "  text = re.sub(r'\\&amp', ' ', text)\n",
    "  text = re.sub(r'\\&lt', ' ', text)\n",
    "  text = re.sub(r'\\&gt', ' ', text)\n",
    "  text = re.sub(r'\\#follow|\\#followme|\\#like|\\#f4f|\\#photooftheday', ' ', text)\n",
    "  return text\n",
    "\n",
    "def remove_slangs(text):\n",
    "  text = re.sub(r' b4 ', ' before ', text)\n",
    "  text = re.sub(r' 2b ', ' to be ', text)\n",
    "  text = re.sub(r' 2morrow ', ' tomorrow ', text)\n",
    "  text = re.sub(r' rn ', ' right now ', text)\n",
    "  text = re.sub(r' brb ', ' be right back ', text)\n",
    "  text = re.sub(r' mb ', ' my bad ', text)\n",
    "  text = re.sub(r' luv ', ' love ', text)\n",
    "  text = re.sub(r' b ', ' be ', text)\n",
    "  text = re.sub(r' r ', ' are ', text)\n",
    "  text = re.sub(r' u ', ' you ', text)\n",
    "  text = re.sub(r' y ', ' why ', text)\n",
    "  text = re.sub(r' ur ', ' your ', text)\n",
    "  text = re.sub(r' hbd ', ' happy birthday ', text)\n",
    "  text = re.sub(r' bday ', ' birthday ', text)\n",
    "  text = re.sub(r' bihday ', ' birthday ', text)\n",
    "  text = re.sub(r' omg ', ' oh my god ', text)\n",
    "  text = re.sub(r' lol ', ' laughing out loud ', text)\n",
    "  return text\n",
    "\n",
    "def remove_abbreviations(text):\n",
    "  text = re.sub(r\" can\\'t \", \" can not \", text)\n",
    "  text = re.sub(r\" i\\'m \", \" i am \", text)\n",
    "  text = re.sub(r\" i\\'ll \", \" i will \", text)\n",
    "  text = re.sub(r\" i\\'d \", \" i would \", text)\n",
    "  text = re.sub(r\" i\\'ve \", \" i have \", text)\n",
    "  text = re.sub(r\" ain\\'t \", \" am not \", text)\n",
    "  text = re.sub(r\" haven\\'t \", \" have not \", text)\n",
    "  text = re.sub(r\" hasn\\'t \", \" has not \", text)\n",
    "  text = re.sub(r\" can\\'t \", \" can not \", text)\n",
    "  text = re.sub(r\" won\\'t \", \" will not \", text)\n",
    "  text = re.sub(r\" you\\'re \", \" you are \", text)\n",
    "  text = re.sub(r\" we\\'re \", \" we are \", text)\n",
    "  text = re.sub(r\" they\\'re \", \" they are \", text)\n",
    "  text = re.sub(r\" he\\'s \", \" he is \", text)\n",
    "  text = re.sub(r\" she\\'s \", \" she is \", text)\n",
    "  text = re.sub(r\" it\\'s \", \" it is \", text)\n",
    "  text = re.sub(r\" don\\'t \", \" do not \", text)\n",
    "  text = re.sub(r\" doesn\\'t \", \" does not \", text)\n",
    "  text = re.sub(r\" wouldn\\'t \", \" would not \", text)\n",
    "  text = re.sub(r\" couldn\\'t \", \" could not \", text)\n",
    "  text = re.sub(r\" shouldn\\'t \", \" should not \", text)\n",
    "  return text\n",
    "\n",
    "def remove_one_len_word(text):\n",
    "  text = re.sub(r'\\b[a-z]\\b', ' ', text)\n",
    "  return text\n",
    "\n",
    "def preprocessing(data):\n",
    "  data['cleaned_tweet'] = data['tweet'].apply(str)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(lambda x: x.lower())\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_newline)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_hyperlink)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_spam)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_link)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_username)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_abbreviations)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(only_letters)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_accent)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_slangs)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_stopwords)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_one_len_word)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(adjustment_text)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab351386",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_data = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7dc96d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce90019c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>father dysfunctional selfish drags kids dysfun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>thanks lyft credit use cause offer wheelchair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>birthday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>model love take time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>huge fan fare big talking leave chaos pay disp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>camping tomorrow dannya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>next school year year exams think school exams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>love land allin cavs champions cleveland cleve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>welcome gr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>ireland consumer price index mom climbed previ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>selfish orlando standwithorlando pulseshooting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>get see daddy today days gettingfed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>cnn calls michigan middle school build wall ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>comment australia opkillingbay seashepherd hel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  label                                              tweet\n",
       "0    1      0  father dysfunctional selfish drags kids dysfun...\n",
       "1    2      0  thanks lyft credit use cause offer wheelchair ...\n",
       "2    3      0                                   birthday majesty\n",
       "3    4      0                               model love take time\n",
       "4    5      0                      factsguide society motivation\n",
       "5    6      0  huge fan fare big talking leave chaos pay disp...\n",
       "6    7      0                            camping tomorrow dannya\n",
       "7    8      0  next school year year exams think school exams...\n",
       "8    9      0  love land allin cavs champions cleveland cleve...\n",
       "9   10      0                                         welcome gr\n",
       "10  11      0  ireland consumer price index mom climbed previ...\n",
       "11  12      0  selfish orlando standwithorlando pulseshooting...\n",
       "12  13      0                get see daddy today days gettingfed\n",
       "13  14      1  cnn calls michigan middle school build wall ch...\n",
       "14  15      1  comment australia opkillingbay seashepherd hel..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = preprocessing(data)\n",
    "data = data.drop_duplicates()\n",
    "data = data.dropna()\n",
    "data = data.drop(columns=['tweet'])\n",
    "data = data.rename(columns={'cleaned_tweet': 'tweet'})\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed41afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(data['tweet'])\n",
    "data['tokenized'] = tokenizer.texts_to_sequences(data['tweet'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf99b743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38843\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80fb7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        embedding_matrix[i] = np.random.randn(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bac7e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38843"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5933c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"word2vec_preprocessing.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(embedding_matrix, output_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc014fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d16a61dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>[1, 34, 4, 252, 11, 16533, 7, 11, 20, 2753, 97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>[1, 1, 170, 9, 5701, 2600, 5, 68, 452, 680, 69...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[62, 26, 3422]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>[140, 5, 13, 37, 76, 16, 37, 21, 2, 39, 8, 162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>[2946, 1621, 49, 299]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1      0   @user when a father is dysfunctional and is s...   \n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3      0                                bihday your majesty   \n",
       "3   4      0  #model   i love u take with u all the time in ...   \n",
       "4   5      0             factsguide: society now    #motivation   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [1, 34, 4, 252, 11, 16533, 7, 11, 20, 2753, 97...  \n",
       "1  [1, 1, 170, 9, 5701, 2600, 5, 68, 452, 680, 69...  \n",
       "2                                     [62, 26, 3422]  \n",
       "3  [140, 5, 13, 37, 76, 16, 37, 21, 2, 39, 8, 162...  \n",
       "4                              [2946, 1621, 49, 299]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = pd.concat([old_data, data], axis=0)\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbbc37d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49175"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(all_data['tweet'].values)\n",
    "all_data['tokenized'] = tokenizer.texts_to_sequences(all_data['tweet'].values)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f2688c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49175\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ec2bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = model[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except:\n",
    "        embedding_matrix[i] = np.random.randn(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a8c23b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49175"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc5f3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"word2vec_total.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(embedding_matrix, output_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
