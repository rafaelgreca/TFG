{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"cnn.ipynb","provenance":[],"collapsed_sections":["laden-parade","r0rq3djpCXxH","59ipy94TLMcs","8r0S-aYwLrSS","XBz-y8sEFvzU","jnO19TNnFzNV","LZUD_tmjqIw3"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"cells":[{"cell_type":"code","metadata":{"id":"-mHbP_8M8Abx","executionInfo":{"status":"ok","timestamp":1628391077871,"user_tz":180,"elapsed":20941,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["%%capture\n","!pip install pandas\n","!pip install tensorflow\n","!pip install -U scikit-learn\n","!pip install matplotlib\n","!pip install seaborn\n","!pip install numpy\n","!pip install nltk\n","!pip install unidecode"],"id":"-mHbP_8M8Abx","execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTgtwqwT8XFa"},"source":["from google.colab import files\n","uploaded = files.upload()"],"id":"kTgtwqwT8XFa"},{"cell_type":"markdown","metadata":{"id":"MUbg5wzUtCwo"},"source":["# Bibliotecas"],"id":"MUbg5wzUtCwo"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"intelligent-input","executionInfo":{"status":"ok","timestamp":1628391079899,"user_tz":180,"elapsed":2052,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"7add93d2-5032-4b3b-ba5d-8e5bf4162ff2"},"source":["import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","import unidecode\n","import warnings\n","import seaborn as sns\n","import time\n","import pickle\n","import nltk\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense, Embedding\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.callbacks import EarlyStopping\n","\n","sw = set(stopwords.words('english'))\n","es = EarlyStopping(patience = 3)\n","np.random.seed(23)\n","sns.set_style('dark')\n","warnings.filterwarnings('ignore')"],"id":"intelligent-input","execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3uuhfExasoOR"},"source":["# Preprocessing"],"id":"3uuhfExasoOR"},{"cell_type":"code","metadata":{"id":"1u-77-IvGw0r","executionInfo":{"status":"ok","timestamp":1628391079904,"user_tz":180,"elapsed":23,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["with open(r\"glove_300d.pickle\", \"rb\") as input_file:\n","  glove = pickle.load(input_file)"],"id":"1u-77-IvGw0r","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"lujWAsrzG-33","executionInfo":{"status":"ok","timestamp":1628391080417,"user_tz":180,"elapsed":530,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["with open(r\"glove_300d_preprocessing.pickle\", \"rb\") as input_file:\n","  glove_preprocessing = pickle.load(input_file)"],"id":"lujWAsrzG-33","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Epj-NAcWs_CF","executionInfo":{"status":"ok","timestamp":1628391080420,"user_tz":180,"elapsed":42,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["def remove_username(text):\n","  text = re.sub(r'\\@[^\\s]+', ' ', text)\n","  return text\n","\n","def remove_newline(text):\n","  text = text.replace('\\n', ' ')\n","  return text\n","\n","def only_letters(text):\n","  text = re.sub(r'[^A-Za-z]+', ' ', text)\n","  return text\n","\n","def remove_link(text):\n","  text = re.sub(r'www\\.?[^\\s]+', ' ', text)\n","  return text\n","\n","def remove_hyperlink(text):\n","  text = re.sub(r'\\<.?\\>', ' ', text)\n","  return text\n","\n","def remove_accent(text):\n","  text = unidecode.unidecode(text)\n","  return text\n","\n","def adjustment_text(text):\n","  text = re.sub(r'\\s+', ' ', text)\n","  text = text.strip()\n","  return text\n","\n","def remove_stopwords(text):\n","  text = [word for word in text.split() if word not in sw]\n","  text = ' '.join(text)\n","  return text\n","\n","def remove_spam(text):\n","  text = re.sub(r'\\&amp', ' ', text)\n","  text = re.sub(r'\\&lt', ' ', text)\n","  text = re.sub(r'\\&gt', ' ', text)\n","  text = re.sub(r'\\#follow|\\#followme|\\#like|\\#f4f|\\#photooftheday', ' ', text)\n","  return text\n","\n","def remove_slangs(text):\n","  text = re.sub(r' b4 ', ' before ', text)\n","  text = re.sub(r' 2b ', ' to be ', text)\n","  text = re.sub(r' 2morrow ', ' tomorrow ', text)\n","  text = re.sub(r' rn ', ' right now ', text)\n","  text = re.sub(r' brb ', ' be right back ', text)\n","  text = re.sub(r' mb ', ' my bad ', text)\n","  text = re.sub(r' luv ', ' love ', text)\n","  text = re.sub(r' b ', ' be ', text)\n","  text = re.sub(r' r ', ' are ', text)\n","  text = re.sub(r' u ', ' you ', text)\n","  text = re.sub(r' y ', ' why ', text)\n","  text = re.sub(r' ur ', ' your ', text)\n","  text = re.sub(r' hbd ', ' happy birthday ', text)\n","  text = re.sub(r' bday ', ' birthday ', text)\n","  text = re.sub(r' bihday ', ' birthday ', text)\n","  text = re.sub(r' omg ', ' oh my god ', text)\n","  text = re.sub(r' lol ', ' laughing out loud ', text)\n","  return text\n","\n","def remove_abbreviations(text):\n","  text = re.sub(r\" can't \", \" can not \", text)\n","  text = re.sub(r\" i'm \", \" i am \", text)\n","  text = re.sub(r\" i'll \", \" i will \", text)\n","  text = re.sub(r\" i'd \", \" i would \", text)\n","  text = re.sub(r\" i've \", \" i have \", text)\n","  text = re.sub(r\" ain't \", \" am not \", text)\n","  text = re.sub(r\" haven't \", \" have not \", text)\n","  text = re.sub(r\" hasn't \", \" has not \", text)\n","  text = re.sub(r\" can't \", \" can not \", text)\n","  text = re.sub(r\" won't \", \" will not \", text)\n","  text = re.sub(r\" you're \", \" you are \", text)\n","  text = re.sub(r\" we're \", \" we are \", text)\n","  text = re.sub(r\" they're \", \" they are \", text)\n","  text = re.sub(r\" he's \", \" he is \", text)\n","  text = re.sub(r\" she's \", \" she is \", text)\n","  text = re.sub(r\" it's \", \" it is \", text)\n","  text = re.sub(r\" don't \", \" do not \", text)\n","  text = re.sub(r\" doesn't \", \" does not \", text)\n","  text = re.sub(r\" wouldn't \", \" would not \", text)\n","  text = re.sub(r\" couldn't \", \" could not \", text)\n","  text = re.sub(r\" shouldn't \", \" should not \", text)\n","  text = re.sub(r\" no-one \", \" noone \", text)\n","  return text\n","\n","def remove_one_len_word(text):\n","  text = re.sub(r'\\b[a-z]\\b', ' ', text)\n","  return text\n","\n","def preprocessing(data):\n","  data['cleaned_tweet'] = data['tweet'].apply(str)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(lambda x: x.lower())\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_newline)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_hyperlink)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_spam)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_link)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_username)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_accent)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_abbreviations)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(only_letters)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_slangs)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_stopwords)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_one_len_word)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(adjustment_text)\n","  return data"],"id":"Epj-NAcWs_CF","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U84N41FtsxSJ"},"source":["# Modelos"],"id":"U84N41FtsxSJ"},{"cell_type":"markdown","metadata":{"id":"laden-parade"},"source":["## Modelo 1"],"id":"laden-parade"},{"cell_type":"code","metadata":{"id":"rYNseLx7-1NN","executionInfo":{"status":"ok","timestamp":1628391080422,"user_tz":180,"elapsed":40,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["def modelo1(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocessing):\n","  model1 = Sequential()\n","\n","  if embedding == 'glove':\n","\n","    if preprocessing:\n","      model1.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove_preprocessing),\n","                      trainable = False))\n","    else:\n","      model1.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove),\n","                      trainable = False))\n","  else: \n","    model1.add(Embedding(input_dim = vocab_size,\n","                        output_dim = embedding_dim,\n","                        input_length = length_size))\n","    \n","  model1.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model1.add(MaxPooling1D())\n","  model1.add(Flatten())\n","  model1.add(Dropout(rate = 0.2))\n","  model1.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model1.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model1 = model1.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es],\n","                            verbose = 0)\n","  \n","  predicted = (model1.predict(X_validation) > 0.5).astype(\"int32\")\n","  score1 = f1_score(predicted, y_validation, average='weighted')\n","  return model1, score1"],"id":"rYNseLx7-1NN","execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r0rq3djpCXxH"},"source":["## Modelo 2"],"id":"r0rq3djpCXxH"},{"cell_type":"code","metadata":{"id":"a5XTt6U4_dYP","executionInfo":{"status":"ok","timestamp":1628391080425,"user_tz":180,"elapsed":38,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["def modelo2(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess):\n","  model2 = Sequential()\n","\n","  if embedding == 'glove':\n","\n","    if preprocessing:\n","      model2.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove_preprocessing),\n","                      trainable = False))\n","    else:\n","      model2.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove),\n","                      trainable = False))\n","  else: \n","    model2.add(Embedding(input_dim = vocab_size,\n","                        output_dim = embedding_dim,\n","                        input_length = length_size))\n","    \n","  model2.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model2.add(MaxPooling1D())\n","  model2.add(Conv1D(filters = 64,\n","                   kernel_size = 5,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model2.add(MaxPooling1D())\n","  model2.add(Flatten())\n","  model2.add(Dropout(rate = 0.2))\n","  model2.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model2.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model2 = model2.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es],\n","                            verbose = 0)\n","  \n","  predicted = (model2.predict(X_validation) > 0.5).astype(\"int32\")\n","  score2 = f1_score(predicted, y_validation, average='weighted')\n","  return model2, score2"],"id":"a5XTt6U4_dYP","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59ipy94TLMcs"},"source":["## Modelo 3"],"id":"59ipy94TLMcs"},{"cell_type":"code","metadata":{"id":"MqMIGrzdF1SK","executionInfo":{"status":"ok","timestamp":1628391080428,"user_tz":180,"elapsed":37,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["def modelo3(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess):\n","  model3 = Sequential()\n","  \n","  if embedding == 'glove':\n","\n","    if preprocessing:\n","      model3.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove_preprocessing),\n","                      trainable=False))\n","    else:\n","      model3.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove),\n","                      trainable=False))\n","  else: \n","    model3.add(Embedding(input_dim = vocab_size,\n","                        output_dim = embedding_dim,\n","                        input_length = length_size))\n","    \n","  model3.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model3.add(MaxPooling1D())\n","  model3.add(Conv1D(filters = 64,\n","                   kernel_size = 5,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model3.add(MaxPooling1D())\n","  model3.add(Conv1D(filters = 128,\n","                   kernel_size = 7,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model3.add(MaxPooling1D())\n","  model3.add(Flatten())\n","  model3.add(Dropout(rate = 0.2))\n","  model3.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model3.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model3 = model3.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es],\n","                            verbose = 0)\n","  \n","  predicted = (model3.predict(X_validation) > 0.5).astype(\"int32\")\n","  score3 = f1_score(predicted, y_validation, average='weighted')\n","  return model3, score3"],"id":"MqMIGrzdF1SK","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8r0S-aYwLrSS"},"source":["## Modelo 4"],"id":"8r0S-aYwLrSS"},{"cell_type":"code","metadata":{"id":"wxtWuv4EAACr","executionInfo":{"status":"ok","timestamp":1628391080429,"user_tz":180,"elapsed":34,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["def modelo4(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess):\n","  model4 = Sequential()\n","  \n","  if embedding == 'glove':\n","\n","    if preprocessing:\n","      model4.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove_preprocessing),\n","                      trainable=False))\n","    else:\n","      model4.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove),\n","                      trainable=False))\n","  else: \n","    model4.add(Embedding(input_dim = vocab_size,\n","                        output_dim = embedding_dim,\n","                        input_length = length_size))\n","    \n","  model4.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model4.add(MaxPooling1D())\n","  model4.add(Conv1D(filters = 64,\n","                   kernel_size = 5,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model4.add(MaxPooling1D())\n","  model4.add(Flatten())\n","  model4.add(Dropout(rate = 0.2))\n","  model4.add(Dense(units = 64,\n","                  activation = 'relu'))\n","  model4.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model4.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model4 = model4.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es],\n","                            verbose = 0)\n","  \n","  predicted = (model4.predict(X_validation) > 0.5).astype(\"int32\")\n","  score4 = f1_score(predicted, y_validation, average='weighted')\n","  return model4, score4"],"id":"wxtWuv4EAACr","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XBz-y8sEFvzU"},"source":["## Modelo 5"],"id":"XBz-y8sEFvzU"},{"cell_type":"code","metadata":{"id":"zKAb-9jC_ut1","executionInfo":{"status":"ok","timestamp":1628391080430,"user_tz":180,"elapsed":33,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["def modelo5(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess):\n","  model5 = Sequential()\n","  \n","  if embedding == 'glove':\n","\n","    if preprocessing:\n","      model5.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove_preprocessing),\n","                      trainable=False,))\n","    else:\n","      model5.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove),\n","                      trainable=False))\n","  else: \n","    model5.add(Embedding(input_dim = vocab_size,\n","                        output_dim = embedding_dim,\n","                        input_length = length_size))\n","    \n","  model5.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model5.add(MaxPooling1D())\n","  model5.add(Flatten())\n","  model5.add(Dropout(rate = 0.2))\n","  model5.add(Dense(units = 64,\n","                  activation = 'relu'))\n","  model5.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model5.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model5 = model5.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es],\n","                            verbose = 0)\n","  \n","  predicted = (model5.predict(X_validation) > 0.5).astype(\"int32\")\n","  score5 = f1_score(predicted, y_validation, average='weighted')\n","  return model5, score5"],"id":"zKAb-9jC_ut1","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jnO19TNnFzNV"},"source":["## Modelo 6"],"id":"jnO19TNnFzNV"},{"cell_type":"code","metadata":{"id":"Ci7HBImnF12-","executionInfo":{"status":"ok","timestamp":1628391080765,"user_tz":180,"elapsed":366,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["def modelo6(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess):\n","  model6 = Sequential()\n","  \n","  if embedding == 'glove':\n","\n","    if preprocessing:\n","      model6.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove_preprocessing),\n","                      trainable=False))\n","    else:\n","      model6.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size,\n","                      embeddings_initializer = keras.initializers.Constant(glove),\n","                      trainable=False))\n","  else: \n","    model6.add(Embedding(input_dim = vocab_size,\n","                        output_dim = embedding_dim,\n","                        input_length = length_size))\n","    \n","  model6.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model6.add(MaxPooling1D())\n","  model6.add(Conv1D(filters = 64,\n","                   kernel_size = 5,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model6.add(MaxPooling1D())\n","  model6.add(Conv1D(filters = 128,\n","                   kernel_size = 7,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model6.add(MaxPooling1D())\n","  model6.add(Flatten())\n","  model6.add(Dropout(rate = 0.2))\n","  model6.add(Dense(units = 64,\n","                  activation = 'relu'))\n","  model6.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model6.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model6 = model6.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es],\n","                            verbose = 0)\n","  \n","  predicted = (model6.predict(X_validation) > 0.5).astype(\"int32\")\n","  score6 = f1_score(predicted, y_validation, average='weighted')\n","  return model6, score6"],"id":"Ci7HBImnF12-","execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZUD_tmjqIw3"},"source":["## Predict"],"id":"LZUD_tmjqIw3"},{"cell_type":"code","metadata":{"id":"ciDWb9FlAcxa","executionInfo":{"status":"ok","timestamp":1628391080781,"user_tz":180,"elapsed":33,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["def predict(model, nome, use_preprocessing, tokenizer, embedding):\n","  test = pd.read_csv('Data/test.csv')\n","\n","  if use_preprocessing:\n","    test = preprocessing(test)\n","    test['tokenized'] = tokenizer.texts_to_sequences(test['cleaned_tweet'])\n","  else:\n","    test['tokenized'] = tokenizer.texts_to_sequences(test['tweet'])\n","\n","  X_test = pad_sequences(sequences = test['tokenized'],\n","                         maxlen = length_size,\n","                         padding = 'post')\n","\n","  predicted = (model.predict(X_test) > 0.5).astype(\"int32\")\n","  prediction = pd.DataFrame()\n","  prediction['id'] = test['id']\n","  prediction['label'] = predicted\n","\n","  if use_preprocessing:\n","\n","    if embedding == 'glove':\n","      prediction.to_csv('Submission/' + nome + '_preprocessamento_glove.csv', index=False)\n","    else:\n","      prediction.to_csv('Submission/' + nome + '_preprocessamento_normal.csv', index=False)\n","  else:\n","\n","    if embedding == 'glove':\n","      prediction.to_csv('Submission/' + nome + '_glove.csv', index=False)\n","    else:\n","      prediction.to_csv('Submission/' + nome + '_normal.csv', index=False)"],"id":"ciDWb9FlAcxa","execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_h6GOY30Bm5N"},"source":["# Main"],"id":"_h6GOY30Bm5N"},{"cell_type":"code","metadata":{"id":"tmFRaTT4WFtK","executionInfo":{"status":"ok","timestamp":1628391080782,"user_tz":180,"elapsed":33,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["def preprocessing_step(use_preprocessing, test_size, data):\n","  tokenizer = Tokenizer()\n","\n","  if use_preprocessing:\n","    tokenizer.fit_on_texts(data['cleaned_tweet'])\n","    data['tokenized'] = tokenizer.texts_to_sequences(data['cleaned_tweet'])\n","  \n","  else:\n","    tokenizer.fit_on_texts(data['tweet'])\n","    data['tokenized'] = tokenizer.texts_to_sequences(data['tweet'])\n","\n","  vocab_size = len(tokenizer.word_index) + 1\n","\n","  X = pad_sequences(sequences = data['tokenized'],\n","                  maxlen = length_size,\n","                  padding = 'post')\n","\n","  y = data['label']\n","\n","  X_train, X_validation, y_train, y_validation = train_test_split(X,\n","                                                                  y, \n","                                                                  test_size = test_size,\n","                                                                  random_state = 23)\n","  \n","  return X_train, X_validation, y_train, y_validation, vocab_size, tokenizer"],"id":"tmFRaTT4WFtK","execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWYSWdGNBpAI","executionInfo":{"status":"ok","timestamp":1628391080784,"user_tz":180,"elapsed":32,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["test_size = 0.15\n","length_size = 15\n","embedding_dim = 300"],"id":"FWYSWdGNBpAI","execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"outside-boards","executionInfo":{"status":"ok","timestamp":1628391080787,"user_tz":180,"elapsed":30,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"42d411de-0282-4a60-be0d-caadab3a926e"},"source":["data = pd.read_csv('Data/train.csv')\n","data = data.drop(columns=['id'])\n","data.head()"],"id":"outside-boards","execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                              tweet\n","0      0   @user when a father is dysfunctional and is s...\n","1      0  @user @user thanks for #lyft credit i can't us...\n","2      0                                bihday your majesty\n","3      0  #model   i love u take with u all the time in ...\n","4      0             factsguide: society now    #motivation"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"Y1iBXFt325sx","executionInfo":{"status":"ok","timestamp":1628391083293,"user_tz":180,"elapsed":2531,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"96a7e638-8d85-4eea-d533-d18d65b79814"},"source":["data = preprocessing(data)\n","data = data.dropna()\n","data.head(15)"],"id":"Y1iBXFt325sx","execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","      <th>cleaned_tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","      <td>father dysfunctional selfish drags kids dysfun...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","      <td>thanks lyft credit use cause offer wheelchair ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","      <td>birthday majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","      <td>model love take time urd ddddd</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","      <td>factsguide society motivation</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>[2/2] huge fan fare and big talking before the...</td>\n","      <td>huge fan fare big talking leave chaos pay disp...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>@user camping tomorrow @user @user @user @use...</td>\n","      <td>camping tomorrow dannya</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>the next school year is the year for exams.ð...</td>\n","      <td>next school year year exams think school exams...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n","      <td>love land allin cavs champions cleveland cleve...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n","      <td>welcome gr</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0</td>\n","      <td>â #ireland consumer price index (mom) climb...</td>\n","      <td>ireland consumer price index mom climbed previ...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0</td>\n","      <td>we are so selfish. #orlando #standwithorlando ...</td>\n","      <td>selfish orlando standwithorlando pulseshooting...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>i get to see my daddy today!!   #80days #getti...</td>\n","      <td>get see daddy today days gettingfed</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>@user #cnn calls #michigan middle school 'buil...</td>\n","      <td>cnn calls michigan middle school build wall ch...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1</td>\n","      <td>no comment!  in #australia   #opkillingbay #se...</td>\n","      <td>comment australia opkillingbay seashepherd hel...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    label  ...                                      cleaned_tweet\n","0       0  ...  father dysfunctional selfish drags kids dysfun...\n","1       0  ...  thanks lyft credit use cause offer wheelchair ...\n","2       0  ...                                   birthday majesty\n","3       0  ...                     model love take time urd ddddd\n","4       0  ...                      factsguide society motivation\n","5       0  ...  huge fan fare big talking leave chaos pay disp...\n","6       0  ...                            camping tomorrow dannya\n","7       0  ...  next school year year exams think school exams...\n","8       0  ...  love land allin cavs champions cleveland cleve...\n","9       0  ...                                         welcome gr\n","10      0  ...  ireland consumer price index mom climbed previ...\n","11      0  ...  selfish orlando standwithorlando pulseshooting...\n","12      0  ...                get see daddy today days gettingfed\n","13      1  ...  cnn calls michigan middle school build wall ch...\n","14      1  ...  comment australia opkillingbay seashepherd hel...\n","\n","[15 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBB0Hsb03DCB","executionInfo":{"status":"ok","timestamp":1628391083296,"user_tz":180,"elapsed":17,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"af0186a5-42b1-4f39-dc6c-e1b8df5e7d77"},"source":["data.shape"],"id":"vBB0Hsb03DCB","execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(31962, 3)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"htgrsK-kELj3","executionInfo":{"status":"ok","timestamp":1628391324381,"user_tz":180,"elapsed":241097,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"1de0dbd0-af75-459d-d9f0-6a5025713abd"},"source":["modelos = ['modelo1']\n","embeddings = ['normal', 'glove']\n","using_preprocessing = [False]\n","use_preprocessing = []\n","modelos_usados = []\n","embeddings_usados = []\n","tempos = []\n","resultados = pd.DataFrame()\n","scores = []\n","\n","print(\"Treinamento iniciado!\\n\")\n","\n","for modelo in modelos:\n","\n","  for preprocess in using_preprocessing:\n","\n","    for embedding in embeddings:\n","\n","      X_train, X_validation, y_train, y_validation, vocab_size, tokenizer = preprocessing_step(preprocess, test_size, data)\n","\n","      print(\"------ \\nTreinando o modelo: {}\\nUsando pré-processamento: {}\\nUsando o embedding: {}\\n------\".format(modelo, preprocess, embedding))\n","\n","      ini = time.time()\n","\n","      if modelo == 'modelo1':\n","        m, score = modelo1(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess)\n","\n","      elif modelo == 'modelo2':\n","        m, score = modelo2(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess)\n","        \n","      elif modelo == 'modelo3':\n","        m, score = modelo3(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess)\n","        \n","      elif modelo == 'modelo4':\n","        m, score = modelo4(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess)\n","        \n","      elif modelo == 'modelo5':\n","        m, score = modelo5(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess)\n","        \n","      elif modelo == 'modelo6':\n","        m, score = modelo6(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, embedding, preprocess)\n","      \n","      fim = time.time()\n","      tempo = round(fim - ini, 3)\n","      tempos.append(tempo)\n","      print(\"\\nTreinamento finalizado em {} segundos!\".format(tempo))\n","      print()\n","\n","      embeddings_usados.append(embedding)\n","      scores.append(score)\n","      modelos_usados.append(modelo)\n","      use_preprocessing.append(preprocess)\n","      predict(m, modelo, preprocess, tokenizer, embedding)"],"id":"htgrsK-kELj3","execution_count":18,"outputs":[{"output_type":"stream","text":["Treinamento iniciado!\n","\n","------ \n","Treinando o modelo: modelo1\n","Usando pré-processamento: False\n","Usando o embedding: normal\n","------\n","\n","Treinamento finalizado em 217.21 segundos!\n","\n","------ \n","Treinando o modelo: modelo1\n","Usando pré-processamento: False\n","Usando o embedding: glove\n","------\n","\n","Treinamento finalizado em 17.68 segundos!\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"id":"bc9XSXUBIRCg","executionInfo":{"status":"ok","timestamp":1628391324385,"user_tz":180,"elapsed":34,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"71227453-e473-4d10-f68e-04c226b2115a"},"source":["resultados['modelo'] = modelos_usados\n","resultados['tempo (s)'] = tempos\n","resultados['f1_score'] = scores\n","resultados['preprocessamento'] = use_preprocessing\n","resultados['embedding'] = embeddings_usados\n","resultados.head(6)"],"id":"bc9XSXUBIRCg","execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>modelo</th>\n","      <th>tempo (s)</th>\n","      <th>f1_score</th>\n","      <th>preprocessamento</th>\n","      <th>embedding</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>modelo1</td>\n","      <td>217.21</td>\n","      <td>0.964925</td>\n","      <td>False</td>\n","      <td>normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>modelo1</td>\n","      <td>17.68</td>\n","      <td>0.960894</td>\n","      <td>False</td>\n","      <td>glove</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    modelo  tempo (s)  f1_score  preprocessamento embedding\n","0  modelo1     217.21  0.964925             False    normal\n","1  modelo1      17.68  0.960894             False     glove"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"Jc8KrT0twfmb","executionInfo":{"status":"ok","timestamp":1628391324387,"user_tz":180,"elapsed":29,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}}},"source":["resultados.to_csv('teste_embeddings.csv', index=False)"],"id":"Jc8KrT0twfmb","execution_count":20,"outputs":[]}]}