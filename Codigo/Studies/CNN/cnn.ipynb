{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"cnn.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"-mHbP_8M8Abx"},"source":["%%capture\n","!pip install pandas\n","!pip install tensorflow\n","!pip install -U scikit-learn\n","!pip install matplotlib\n","!pip install seaborn\n","!pip install numpy\n","!pip install nltk\n","!pip install unidecode"],"id":"-mHbP_8M8Abx","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTgtwqwT8XFa"},"source":["from google.colab import files\n","uploaded = files.upload()"],"id":"kTgtwqwT8XFa"},{"cell_type":"markdown","metadata":{"id":"MUbg5wzUtCwo"},"source":["# Bibliotecas"],"id":"MUbg5wzUtCwo"},{"cell_type":"code","metadata":{"id":"intelligent-input","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628302948550,"user_tz":180,"elapsed":2554,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"d2de4dfc-9cf0-4585-aec1-1877a8ce79af"},"source":["import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","import unidecode\n","import warnings\n","import seaborn as sns\n","import nltk\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense, Embedding\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.callbacks import EarlyStopping\n","\n","sw = set(stopwords.words('english'))\n","es = EarlyStopping(patience = 3)\n","np.random.seed(23)\n","sns.set_style('dark')\n","warnings.filterwarnings('ignore')"],"id":"intelligent-input","execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3uuhfExasoOR"},"source":["# Preprocessing"],"id":"3uuhfExasoOR"},{"cell_type":"code","metadata":{"id":"Epj-NAcWs_CF"},"source":["def remove_username(text):\n","  text = re.sub(r'\\@[^\\s]+', ' ', text)\n","  return text\n","\n","def remove_newline(text):\n","  text = text.replace('\\n', ' ')\n","  return text\n","\n","def only_letters(text):\n","  text = re.sub(r'[^A-Za-z]+', ' ', text)\n","  return text\n","\n","def remove_link(text):\n","  text = re.sub(r'www\\.?[^\\s]+', ' ', text)\n","  return text\n","\n","def remove_hyperlink(text):\n","  text = re.sub(r'\\<.?\\>', ' ', text)\n","  return text\n","\n","def remove_accent(text):\n","  text = unidecode.unidecode(text)\n","  return text\n","\n","def adjustment_text(text):\n","  text = re.sub(r'\\s+', ' ', text)\n","  text = text.strip()\n","  return text\n","\n","def remove_stopwords(text):\n","  text = [word for word in text.split() if word not in sw]\n","  text = ' '.join(text)\n","  return text\n","\n","def remove_spam(text):\n","  text = re.sub(r'\\&amp', ' ', text)\n","  text = re.sub(r'\\&lt', ' ', text)\n","  text = re.sub(r'\\&gt', ' ', text)\n","  text = re.sub(r'\\#follow|\\#followme|\\#like|\\#f4f|\\#photooftheday', ' ', text)\n","  return text\n","\n","def remove_slangs(text):\n","  text = re.sub(r' b4 ', ' before ', text)\n","  text = re.sub(r' 2b ', ' to be ', text)\n","  text = re.sub(r' 2morrow ', ' tomorrow ', text)\n","  text = re.sub(r' rn ', ' right now ', text)\n","  text = re.sub(r' brb ', ' be right back ', text)\n","  text = re.sub(r' mb ', ' my bad ', text)\n","  text = re.sub(r' luv ', ' love ', text)\n","  text = re.sub(r' b ', ' be ', text)\n","  text = re.sub(r' r ', ' are ', text)\n","  text = re.sub(r' u ', ' you ', text)\n","  text = re.sub(r' y ', ' why ', text)\n","  text = re.sub(r' ur ', ' your ', text)\n","  text = re.sub(r' hbd ', ' happy birthday ', text)\n","  text = re.sub(r' bday ', ' birthday ', text)\n","  text = re.sub(r' bihday ', ' birthday ', text)\n","  text = re.sub(r' omg ', ' oh my god ', text)\n","  text = re.sub(r' lol ', ' laughing out loud ', text)\n","  return text\n","\n","def remove_abbreviations(text):\n","  text = re.sub(r\" can't \", \" can not \", text)\n","  text = re.sub(r\" i'm \", \" i am \", text)\n","  text = re.sub(r\" i'll \", \" i will \", text)\n","  text = re.sub(r\" i'd \", \" i would \", text)\n","  text = re.sub(r\" i've \", \" i have \", text)\n","  text = re.sub(r\" ain't \", \" am not \", text)\n","  text = re.sub(r\" haven't \", \" have not \", text)\n","  text = re.sub(r\" hasn't \", \" has not \", text)\n","  text = re.sub(r\" can't \", \" can not \", text)\n","  text = re.sub(r\" won't \", \" will not \", text)\n","  text = re.sub(r\" you're \", \" you are \", text)\n","  text = re.sub(r\" we're \", \" we are \", text)\n","  text = re.sub(r\" they're \", \" they are \", text)\n","  text = re.sub(r\" he's \", \" he is \", text)\n","  text = re.sub(r\" she's \", \" she is \", text)\n","  text = re.sub(r\" it's \", \" it is \", text)\n","  text = re.sub(r\" don't \", \" do not \", text)\n","  text = re.sub(r\" doesn't \", \" does not \", text)\n","  text = re.sub(r\" wouldn't \", \" would not \", text)\n","  text = re.sub(r\" couldn't \", \" could not \", text)\n","  text = re.sub(r\" shouldn't \", \" should not \", text)\n","  text = re.sub(r\" no-one \", \" noone \", text)\n","  return text\n","\n","def remove_one_len_word(text):\n","  text = re.sub(r'\\b[a-z]\\b', ' ', text)\n","  return text\n","\n","def preprocessing(data):\n","  data['cleaned_tweet'] = data['tweet'].apply(str)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(lambda x: x.lower())\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_newline)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_hyperlink)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_spam)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_link)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_username)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_accent)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_abbreviations)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(only_letters)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_slangs)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_stopwords)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_one_len_word)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(adjustment_text)\n","  return data"],"id":"Epj-NAcWs_CF","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U84N41FtsxSJ"},"source":["# Modelos"],"id":"U84N41FtsxSJ"},{"cell_type":"markdown","metadata":{"id":"laden-parade"},"source":["## Modelo 1"],"id":"laden-parade"},{"cell_type":"code","metadata":{"id":"rYNseLx7-1NN"},"source":["def modelo1(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size):\n","  model1 = Sequential()\n","  model1.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size))\n","  model1.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model1.add(MaxPooling1D())\n","  model1.add(Flatten())\n","  model1.add(Dropout(rate = 0.2))\n","  model1.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model1.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model1 = model1.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es])\n","  \n","  predicted = (model1.predict(X_validation) > 0.5).astype(\"int32\")\n","  score1 = f1_score(predicted, y_validation, average='weighted')\n","  return model1, score1"],"id":"rYNseLx7-1NN","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r0rq3djpCXxH"},"source":["## Modelo 2"],"id":"r0rq3djpCXxH"},{"cell_type":"code","metadata":{"id":"a5XTt6U4_dYP"},"source":["def modelo2(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size):\n","  model2 = Sequential()\n","  model2.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size))\n","  model2.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model2.add(MaxPooling1D())\n","  model2.add(Conv1D(filters = 64,\n","                   kernel_size = 5,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model2.add(MaxPooling1D())\n","  model2.add(Flatten())\n","  model2.add(Dropout(rate = 0.2))\n","  model2.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model2.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model2 = model2.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es])\n","  \n","  predicted = (model2.predict(X_validation) > 0.5).astype(\"int32\")\n","  score2 = f1_score(predicted, y_validation, average='weighted')\n","  return model2, score2"],"id":"a5XTt6U4_dYP","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"59ipy94TLMcs"},"source":["## Modelo 3"],"id":"59ipy94TLMcs"},{"cell_type":"code","metadata":{"id":"MqMIGrzdF1SK"},"source":["def modelo3(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size):\n","  model3 = Sequential()\n","  model3.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size))\n","  model3.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model3.add(MaxPooling1D())\n","  model3.add(Conv1D(filters = 64,\n","                   kernel_size = 5,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model3.add(MaxPooling1D())\n","  model3.add(Conv1D(filters = 128,\n","                   kernel_size = 7,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model3.add(MaxPooling1D())\n","  model3.add(Flatten())\n","  model3.add(Dropout(rate = 0.2))\n","  model3.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model3.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model3 = model3.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es])\n","  \n","  predicted = (model3.predict(X_validation) > 0.5).astype(\"int32\")\n","  score3 = f1_score(predicted, y_validation, average='weighted')\n","  return model3, score3"],"id":"MqMIGrzdF1SK","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8r0S-aYwLrSS"},"source":["## Modelo 4"],"id":"8r0S-aYwLrSS"},{"cell_type":"code","metadata":{"id":"wxtWuv4EAACr"},"source":["def modelo4(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size):\n","  model4 = Sequential()\n","  model4.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size))\n","  model4.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model4.add(MaxPooling1D())\n","  model4.add(Conv1D(filters = 64,\n","                   kernel_size = 5,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model4.add(MaxPooling1D())\n","  model4.add(Flatten())\n","  model4.add(Dropout(rate = 0.2))\n","  model4.add(Dense(units = 64,\n","                  activation = 'relu'))\n","  model4.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model4.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model4 = model4.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es])\n","  \n","  predicted = (model4.predict(X_validation) > 0.5).astype(\"int32\")\n","  score4 = f1_score(predicted, y_validation, average='weighted')\n","  return model4, score4"],"id":"wxtWuv4EAACr","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XBz-y8sEFvzU"},"source":["## Modelo 5"],"id":"XBz-y8sEFvzU"},{"cell_type":"code","metadata":{"id":"zKAb-9jC_ut1"},"source":["def modelo5(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size):\n","  model5 = Sequential()\n","  model5.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size))\n","  model5.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model5.add(MaxPooling1D())\n","  model5.add(Flatten())\n","  model5.add(Dropout(rate = 0.2))\n","  model5.add(Dense(units = 64,\n","                  activation = 'relu'))\n","  model5.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model5.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model5 = model5.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es])\n","  \n","  predicted = (model5.predict(X_validation) > 0.5).astype(\"int32\")\n","  score5 = f1_score(predicted, y_validation, average='weighted')\n","  return model5, score5"],"id":"zKAb-9jC_ut1","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jnO19TNnFzNV"},"source":["## Modelo 6"],"id":"jnO19TNnFzNV"},{"cell_type":"code","metadata":{"id":"Ci7HBImnF12-"},"source":["def modelo6(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size):\n","  model6 = Sequential()\n","  model6.add(Embedding(input_dim = vocab_size,\n","                      output_dim = embedding_dim,\n","                      input_length = length_size))\n","  model6.add(Conv1D(filters = 32,\n","                   kernel_size = 3,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model6.add(MaxPooling1D())\n","  model6.add(Conv1D(filters = 64,\n","                   kernel_size = 5,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model6.add(MaxPooling1D())\n","  model6.add(Conv1D(filters = 128,\n","                   kernel_size = 7,\n","                   padding = 'same',\n","                   activation = 'relu'))\n","  model6.add(MaxPooling1D())\n","  model6.add(Flatten())\n","  model6.add(Dropout(rate = 0.2))\n","  model6.add(Dense(units = 64,\n","                  activation = 'relu'))\n","  model6.add(Dense(units = 1,\n","                  activation = 'sigmoid'))\n","  \n","  #model.summary()\n","  model6.compile(optimizer = 'adam',\n","                loss = 'binary_crossentropy',\n","                metrics = ['accuracy'])\n","  \n","  history_model6 = model6.fit(x = X_train,\n","                            y = y_train,\n","                            validation_data = (X_validation, y_validation),\n","                            batch_size = 100,\n","                            epochs = 20,\n","                            callbacks = [es])\n","  \n","  predicted = (model6.predict(X_validation) > 0.5).astype(\"int32\")\n","  score6 = f1_score(predicted, y_validation, average='weighted')\n","  return model6, score6"],"id":"Ci7HBImnF12-","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZUD_tmjqIw3"},"source":["## Predict"],"id":"LZUD_tmjqIw3"},{"cell_type":"code","metadata":{"id":"ciDWb9FlAcxa"},"source":["def predict(model, nome, use_preprocessing, tokenizer):\n","  test = pd.read_csv('Data/test.csv')\n","\n","  if use_preprocessing:\n","    test = preprocessing(test)\n","    test['tokenized'] = tokenizer.texts_to_sequences(test['cleaned_tweet'])\n","  else:\n","    test['tokenized'] = tokenizer.texts_to_sequences(test['tweet'])\n","\n","  X_test = pad_sequences(sequences = test['tokenized'],\n","                         maxlen = length_size,\n","                         padding = 'post')\n","\n","  predicted = (model.predict(X_test) > 0.5).astype(\"int32\")\n","  prediction = pd.DataFrame()\n","  prediction['id'] = test['id']\n","  prediction['label'] = predicted\n","\n","  if use_preprocessing:\n","    prediction.to_csv(nome + '_preprocessamento.csv', index=False)\n","  else:\n","    prediction.to_csv(nome + '.csv', index=False)"],"id":"ciDWb9FlAcxa","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_h6GOY30Bm5N"},"source":["# Main"],"id":"_h6GOY30Bm5N"},{"cell_type":"code","metadata":{"id":"tmFRaTT4WFtK"},"source":["def preprocessing_step(use_preprocessing, test_size, data):\n","  tokenizer = Tokenizer()\n","\n","  if use_preprocessing:\n","    tokenizer.fit_on_texts(data['clean_tweet'])\n","    data['tokenized'] = tokenizer.texts_to_sequences(data['clean_tweet'])\n","  \n","  else:\n","    tokenizer.fit_on_texts(data['tweet'])\n","    data['tokenized'] = tokenizer.texts_to_sequences(data['tweet'])\n","\n","  vocab_size = len(tokenizer.word_index) + 1\n","\n","  X = pad_sequences(sequences = data['tokenized'],\n","                  maxlen = length_size,\n","                  padding = 'post')\n","\n","  y = data['label']\n","\n","  X_train, X_validation, y_train, y_validation = train_test_split(X,\n","                                                                  y, \n","                                                                  test_size = test_size,\n","                                                                  random_state = 23)\n","  \n","  return X_train, X_validation, y_train, y_validation, vocab_size, tokenizer"],"id":"tmFRaTT4WFtK","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWYSWdGNBpAI"},"source":["use_preprocessing = False\n","test_size = 0.15\n","length_size = 15\n","embedding_dim = 300"],"id":"FWYSWdGNBpAI","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"outside-boards","executionInfo":{"status":"ok","timestamp":1628302948960,"user_tz":180,"elapsed":34,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"b6000352-a474-48cd-d080-435440bb7876"},"source":["data = pd.read_csv('Data/train.csv')\n","data = data.drop(columns=['id'])\n","data.head()"],"id":"outside-boards","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                              tweet\n","0      0   @user when a father is dysfunctional and is s...\n","1      0  @user @user thanks for #lyft credit i can't us...\n","2      0                                bihday your majesty\n","3      0  #model   i love u take with u all the time in ...\n","4      0             factsguide: society now    #motivation"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520},"id":"Y1iBXFt325sx","executionInfo":{"status":"ok","timestamp":1628302951821,"user_tz":180,"elapsed":2891,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"c43b849e-ea7c-4914-996e-709efa03ccf1"},"source":["data = preprocessing(data)\n","data = data.dropna()\n","data.head(15)"],"id":"Y1iBXFt325sx","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","      <th>cleaned_tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","      <td>father dysfunctional selfish drags kids dysfun...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","      <td>thanks lyft credit use cause offer wheelchair ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","      <td>birthday majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","      <td>model love take time urd ddddd</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","      <td>factsguide society motivation</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>[2/2] huge fan fare and big talking before the...</td>\n","      <td>huge fan fare big talking leave chaos pay disp...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>@user camping tomorrow @user @user @user @use...</td>\n","      <td>camping tomorrow dannya</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>the next school year is the year for exams.ð...</td>\n","      <td>next school year year exams think school exams...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n","      <td>love land allin cavs champions cleveland cleve...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0</td>\n","      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n","      <td>welcome gr</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0</td>\n","      <td>â #ireland consumer price index (mom) climb...</td>\n","      <td>ireland consumer price index mom climbed previ...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0</td>\n","      <td>we are so selfish. #orlando #standwithorlando ...</td>\n","      <td>selfish orlando standwithorlando pulseshooting...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0</td>\n","      <td>i get to see my daddy today!!   #80days #getti...</td>\n","      <td>get see daddy today days gettingfed</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>@user #cnn calls #michigan middle school 'buil...</td>\n","      <td>cnn calls michigan middle school build wall ch...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1</td>\n","      <td>no comment!  in #australia   #opkillingbay #se...</td>\n","      <td>comment australia opkillingbay seashepherd hel...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    label  ...                                      cleaned_tweet\n","0       0  ...  father dysfunctional selfish drags kids dysfun...\n","1       0  ...  thanks lyft credit use cause offer wheelchair ...\n","2       0  ...                                   birthday majesty\n","3       0  ...                     model love take time urd ddddd\n","4       0  ...                      factsguide society motivation\n","5       0  ...  huge fan fare big talking leave chaos pay disp...\n","6       0  ...                            camping tomorrow dannya\n","7       0  ...  next school year year exams think school exams...\n","8       0  ...  love land allin cavs champions cleveland cleve...\n","9       0  ...                                         welcome gr\n","10      0  ...  ireland consumer price index mom climbed previ...\n","11      0  ...  selfish orlando standwithorlando pulseshooting...\n","12      0  ...                get see daddy today days gettingfed\n","13      1  ...  cnn calls michigan middle school build wall ch...\n","14      1  ...  comment australia opkillingbay seashepherd hel...\n","\n","[15 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBB0Hsb03DCB","executionInfo":{"status":"ok","timestamp":1628302951822,"user_tz":180,"elapsed":14,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"61f9f24d-05de-4068-9093-1aa453c9cb42"},"source":["data.shape"],"id":"vBB0Hsb03DCB","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(31962, 3)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"htgrsK-kELj3","executionInfo":{"status":"ok","timestamp":1628303406851,"user_tz":180,"elapsed":455037,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"ef10b661-46fd-46fb-ca6a-d7a744b2f096"},"source":["modelos = ['modelo1', 'modelo2']\n","resultados = pd.DataFrame()\n","scores = []\n","\n","for modelo in modelos:\n","\n","  X_train, X_validation, y_train, y_validation, vocab_size, tokenizer = preprocessing_step(use_preprocessing, test_size, data)\n","\n","  print(\"------ {} ------\".format(modelo))\n","\n","  if modelo == 'modelo1':\n","    m, score = modelo1(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size)\n","\n","  elif modelo == 'modelo2':\n","    m, score = modelo2(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size)\n","    \n","  elif modelo == 'modelo3':\n","    m, score = modelo3(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size)\n","    \n","  elif modelo == 'modelo4':\n","    m, score = modelo4(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size)\n","    \n","  elif modelo == 'modelo5':\n","    m, score = modelo5(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size)\n","    \n","  elif modelo == 'modelo6':\n","    m, score = modelo6(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size)\n","  \n","  print()\n","  scores.append(score)\n","  predict(m, modelo, use_preprocessing, tokenizer)"],"id":"htgrsK-kELj3","execution_count":null,"outputs":[{"output_type":"stream","text":["------ modelo1 ------\n","Epoch 1/20\n","272/272 [==============================] - 59s 159ms/step - loss: 0.2968 - accuracy: 0.9287 - val_loss: 0.1174 - val_accuracy: 0.9602\n","Epoch 2/20\n","272/272 [==============================] - 43s 157ms/step - loss: 0.0637 - accuracy: 0.9779 - val_loss: 0.1094 - val_accuracy: 0.9620\n","Epoch 3/20\n","272/272 [==============================] - 43s 156ms/step - loss: 0.0192 - accuracy: 0.9950 - val_loss: 0.1239 - val_accuracy: 0.9608\n","Epoch 4/20\n","272/272 [==============================] - 43s 157ms/step - loss: 0.0062 - accuracy: 0.9987 - val_loss: 0.1421 - val_accuracy: 0.9614\n","Epoch 5/20\n","272/272 [==============================] - 42s 156ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.1644 - val_accuracy: 0.9616\n","\n","------ modelo2 ------\n","Epoch 1/20\n","272/272 [==============================] - 45s 160ms/step - loss: 0.2958 - accuracy: 0.9012 - val_loss: 0.1177 - val_accuracy: 0.9593\n","Epoch 2/20\n","272/272 [==============================] - 43s 158ms/step - loss: 0.0606 - accuracy: 0.9784 - val_loss: 0.1155 - val_accuracy: 0.9623\n","Epoch 3/20\n","272/272 [==============================] - 43s 157ms/step - loss: 0.0170 - accuracy: 0.9953 - val_loss: 0.1380 - val_accuracy: 0.9597\n","Epoch 4/20\n","272/272 [==============================] - 43s 159ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.1686 - val_accuracy: 0.9577\n","Epoch 5/20\n","272/272 [==============================] - 43s 158ms/step - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.1959 - val_accuracy: 0.9589\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bc9XSXUBIRCg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628303406854,"user_tz":180,"elapsed":31,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"}},"outputId":"4a2853dc-31d0-4440-8b04-f31a536b430e"},"source":["resultados['modelos'] = modelos\n","resultados['f1_score'] = scores\n","resultados['preprocessamento'] = use_preprocessing\n","resultados.head(6)"],"id":"bc9XSXUBIRCg","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>modelos</th>\n","      <th>f1_score</th>\n","      <th>preprocessamento</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>modelo1</td>\n","      <td>0.964146</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>modelo2</td>\n","      <td>0.960847</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   modelos  f1_score  preprocessamento\n","0  modelo1  0.964146             False\n","1  modelo2  0.960847             False"]},"metadata":{"tags":[]},"execution_count":17}]}]}