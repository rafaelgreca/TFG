{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextCNN_2D","provenance":[{"file_id":"1fwLg3iDRncD7JInzfhJJ_mg4wDpeihHg","timestamp":1629671437352}],"collapsed_sections":[],"authorship_tag":"ABX9TyP0B8DC8BfgvfJ0/oV+ov24"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"MvrZGmWA8LQG"},"source":["%%capture\n","!pip install pandas\n","!pip install numpy\n","!pip install tensorflow\n","!pip install keras\n","!pip install sklearn\n","!pip install matplotlib\n","!pip install seaborn\n","!pip install unidecode\n","!pip install -U imbalanced-learn\n","!pip3 install pickle5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbkU3YVN8Pwd","executionInfo":{"elapsed":3738,"status":"ok","timestamp":1630548036715,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"},"user_tz":180},"outputId":"08bee79b-d6e4-49bf-f942-3b8dba474bd6"},"source":["import tensorflow as tf\n","import pandas as pd\n","import warnings\n","import unidecode\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pickle5 as pickle\n","import random\n","import os\n","import re\n","import nltk\n","nltk.download('stopwords')\n","\n","from imblearn.over_sampling import RandomOverSampler\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from tensorflow import keras\n","from keras.models import Sequential, Model\n","from keras.layers import Reshape, Dense, Dropout, Flatten, Input, MaxPooling2D, Convolution2D, Embedding, Concatenate\n","from keras.regularizers import l2\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.metrics import f1_score\n","\n","sw = set(stopwords.words('english'))\n","os.environ['PYTHONHASHSEED']=str(23)\n","tf.random.set_seed(23)\n","random.seed(23)\n","warnings.filterwarnings('ignore')\n","np.random.seed(23)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"fbIsBrCP1iMA"},"source":["## Preprocessing"]},{"cell_type":"code","metadata":{"id":"DVGCip5q1hla"},"source":["def remove_username(text):\n","  text = re.sub(r'\\@[^\\s]+', ' ', text)\n","  return text\n","\n","def remove_newline(text):\n","  text = text.replace('\\n', ' ')\n","  return text\n","\n","def only_letters(text):\n","  text = re.sub(r'[^a-záâàãéêèẽíìîĩóòõôúùũû\\s]+', ' ', text)\n","  return text\n","\n","def remove_link(text):\n","  text = re.sub(r'www\\.?[^\\s]+', ' ', text)\n","  return text\n","\n","def remove_hyperlink(text):\n","  text = re.sub(r'\\<.?\\>', ' ', text)\n","  return text\n","\n","def remove_accent(text):\n","  text = unidecode.unidecode(text)\n","  return text\n","\n","def adjustment_text(text):\n","  text = re.sub(r'\\s+', ' ', text)\n","  text = text.strip()\n","  return text\n","\n","def remove_stopwords(text):\n","  text = [word for word in text.split() if word not in sw]\n","  text = ' '.join(text)\n","  return text\n","\n","def remove_spam(text):\n","  text = re.sub(r'\\&amp', ' ', text)\n","  text = re.sub(r'\\&lt', ' ', text)\n","  text = re.sub(r'\\&gt', ' ', text)\n","  text = re.sub(r'\\#follow|\\#followme|\\#like|\\#f4f|\\#photooftheday', ' ', text)\n","  return text\n","\n","def remove_slangs(text):\n","  text = re.sub(r' b4 ', ' before ', text)\n","  text = re.sub(r' 2b ', ' to be ', text)\n","  text = re.sub(r' 2morrow ', ' tomorrow ', text)\n","  text = re.sub(r' rn ', ' right now ', text)\n","  text = re.sub(r' brb ', ' be right back ', text)\n","  text = re.sub(r' mb ', ' my bad ', text)\n","  text = re.sub(r' luv ', ' love ', text)\n","  text = re.sub(r' b ', ' be ', text)\n","  text = re.sub(r' r ', ' are ', text)\n","  text = re.sub(r' u ', ' you ', text)\n","  text = re.sub(r' y ', ' why ', text)\n","  text = re.sub(r' ur ', ' your ', text)\n","  text = re.sub(r' hbd ', ' happy birthday ', text)\n","  text = re.sub(r' bday ', ' birthday ', text)\n","  text = re.sub(r' bihday ', ' birthday ', text)\n","  text = re.sub(r' omg ', ' oh my god ', text)\n","  text = re.sub(r' lol ', ' laughing out loud ', text)\n","  return text\n","\n","def remove_abbreviations(text):\n","  text = re.sub(r\" can\\'t \", \" can not \", text)\n","  text = re.sub(r\" i\\'m \", \" i am \", text)\n","  text = re.sub(r\" i\\'ll \", \" i will \", text)\n","  text = re.sub(r\" i\\'d \", \" i would \", text)\n","  text = re.sub(r\" i\\'ve \", \" i have \", text)\n","  text = re.sub(r\" ain\\'t \", \" am not \", text)\n","  text = re.sub(r\" haven\\'t \", \" have not \", text)\n","  text = re.sub(r\" hasn\\'t \", \" has not \", text)\n","  text = re.sub(r\" can\\'t \", \" can not \", text)\n","  text = re.sub(r\" won\\'t \", \" will not \", text)\n","  text = re.sub(r\" you\\'re \", \" you are \", text)\n","  text = re.sub(r\" we\\'re \", \" we are \", text)\n","  text = re.sub(r\" they\\'re \", \" they are \", text)\n","  text = re.sub(r\" he\\'s \", \" he is \", text)\n","  text = re.sub(r\" she\\'s \", \" she is \", text)\n","  text = re.sub(r\" it\\'s \", \" it is \", text)\n","  text = re.sub(r\" don\\'t \", \" do not \", text)\n","  text = re.sub(r\" doesn\\'t \", \" does not \", text)\n","  text = re.sub(r\" wouldn\\'t \", \" would not \", text)\n","  text = re.sub(r\" couldn\\'t \", \" could not \", text)\n","  text = re.sub(r\" shouldn\\'t \", \" should not \", text)\n","  return text\n","\n","def remove_one_len_word(text):\n","  text = re.sub(r'\\b[a-z]\\b', ' ', text)\n","  return text\n","\n","def preprocessing(data):\n","  data['cleaned_tweet'] = data['tweet'].apply(str)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(lambda x: x.lower())\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_newline)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_hyperlink)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_spam)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_link)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_username)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_abbreviations)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(only_letters)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_accent)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_slangs)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_stopwords)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_one_len_word)\n","  data['cleaned_tweet'] = data['cleaned_tweet'].apply(adjustment_text)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wM2uoJUZ8oVr"},"source":["from google.colab import files\n","uploaded = files.upload()"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"0PRNAa9_-qIM","executionInfo":{"elapsed":33,"status":"ok","timestamp":1630548037196,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"},"user_tz":180},"outputId":"9dbd347d-20ac-4bba-f741-b3749e1f0fa3"},"source":["normal_data = pd.read_csv('Data/train.csv')\n","normal_data = normal_data.drop(columns=['id'])\n","normal_data.head()"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                              tweet\n","0      0   @user when a father is dysfunctional and is s...\n","1      0  @user @user thanks for #lyft credit i can't us...\n","2      0                                bihday your majesty\n","3      0  #model   i love u take with u all the time in ...\n","4      0             factsguide: society now    #motivation"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iv3WqTC3kKPP","executionInfo":{"elapsed":32,"status":"ok","timestamp":1630548037199,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"},"user_tz":180},"outputId":"ad4b672b-83b5-444a-ab76-fe4da2914d23"},"source":["normal_data.shape"],"execution_count":null,"outputs":[{"data":{"text/plain":["(31962, 2)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"YWsDE9A2kD_u","executionInfo":{"elapsed":26,"status":"ok","timestamp":1630548037201,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"},"user_tz":180},"outputId":"b93e0e5f-245e-4835-e8e7-3ec85773bd54"},"source":["ros = RandomOverSampler(random_state=23, sampling_strategy='minority')\n","X_resampled, y_resampled = ros.fit_resample(normal_data[['tweet']], normal_data['label'])\n","data_augmentation = pd.concat([X_resampled, y_resampled], axis=1)\n","data_augmentation.head()"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>bihday your majesty</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>#model   i love u take with u all the time in ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>factsguide: society now    #motivation</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               tweet  label\n","0   @user when a father is dysfunctional and is s...      0\n","1  @user @user thanks for #lyft credit i can't us...      0\n","2                                bihday your majesty      0\n","3  #model   i love u take with u all the time in ...      0\n","4             factsguide: society now    #motivation      0"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"szfpHt7rzTIb","executionInfo":{"elapsed":23,"status":"ok","timestamp":1630548037202,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"},"user_tz":180},"outputId":"45bbb860-c3a6-4089-92ba-bd8cc3e2c547"},"source":["data_augmentation.shape"],"execution_count":null,"outputs":[{"data":{"text/plain":["(59440, 2)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"2Y4K9Bt11Zeu","executionInfo":{"elapsed":3108,"status":"ok","timestamp":1630548040772,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"},"user_tz":180},"outputId":"c6251f8c-cc31-445b-9e3b-9426626610cf"},"source":["preprocessed_data = normal_data.copy()\n","preprocessed_data = preprocessing(preprocessed_data)\n","preprocessed_data = preprocessed_data.replace('None', pd.NA)\n","preprocessed_data = preprocessed_data.dropna()\n","preprocessed_data = preprocessed_data.drop_duplicates()\n","preprocessed_data = preprocessed_data.drop(columns=['tweet'])\n","preprocessed_data = preprocessed_data.rename(columns={'cleaned_tweet': 'tweet'})\n","preprocessed_data.head()"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>father dysfunctional selfish drags kids dysfun...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>thanks lyft credit use cause offer wheelchair ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>birthday majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>model love take time</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>factsguide society motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                              tweet\n","0      0  father dysfunctional selfish drags kids dysfun...\n","1      0  thanks lyft credit use cause offer wheelchair ...\n","2      0                                   birthday majesty\n","3      0                               model love take time\n","4      0                      factsguide society motivation"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pTkLD9L4zM0Z","executionInfo":{"elapsed":28,"status":"ok","timestamp":1630548040774,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"},"user_tz":180},"outputId":"8f0d3a82-c564-4fa4-daed-06a021d138f6"},"source":["preprocessed_data.shape"],"execution_count":null,"outputs":[{"data":{"text/plain":["(29530, 2)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"hebFvYTaBpAG","executionInfo":{"elapsed":26,"status":"ok","timestamp":1630548040776,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"},"user_tz":180},"outputId":"d7c6d5d1-95fb-456a-8e05-24881ac1b510"},"source":["ros = RandomOverSampler(random_state=23, sampling_strategy='minority')\n","X_resampled, y_resampled = ros.fit_resample(preprocessed_data[['tweet']], preprocessed_data['label'])\n","data_preprocessing_augmentation = pd.concat([X_resampled, y_resampled], axis=1)\n","data_preprocessing_augmentation.head()"],"execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>father dysfunctional selfish drags kids dysfun...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>thanks lyft credit use cause offer wheelchair ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>birthday majesty</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>model love take time</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>factsguide society motivation</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                               tweet  label\n","0  father dysfunctional selfish drags kids dysfun...      0\n","1  thanks lyft credit use cause offer wheelchair ...      0\n","2                                   birthday majesty      0\n","3                               model love take time      0\n","4                      factsguide society motivation      0"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q1gSgHkFzPOt","executionInfo":{"elapsed":24,"status":"ok","timestamp":1630548040778,"user":{"displayName":"Rafael Greca","photoUrl":"","userId":"13108790060757877730"},"user_tz":180},"outputId":"de82d0d0-e49c-42cb-9fab-e4cecfb5322e"},"source":["data_preprocessing_augmentation.shape"],"execution_count":null,"outputs":[{"data":{"text/plain":["(55034, 2)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"iCpQSNEE7wlo"},"source":["## Word2vec"]},{"cell_type":"code","metadata":{"id":"HmVKbyWJ7y1W"},"source":["with open(r\"Data/word2vec.pickle\", \"rb\") as output_file:\n","    word2vec_embedding = pickle.load(output_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Df2gQAxIsteF"},"source":["with open(r\"Data/word2vec_preprocessing.pickle\", \"rb\") as output_file:\n","    word2vec_preprocessing_embedding = pickle.load(output_file)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2tnMZB6L9Cwf"},"source":["## Parameters"]},{"cell_type":"code","metadata":{"id":"RLPjaO0_9HRS"},"source":["non_linearity_function = 'relu'\n","kernel_size = [3, 4, 5]\n","filters = 100\n","dropout_rate = 0.5\n","l2_constraint = 3\n","epochs = 10\n","batch_size = 100\n","embedding_dim = 300\n","length_size = 30"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xgf0e9zIM_Ee"},"source":["## Tokenization + padding + splitting data step"]},{"cell_type":"code","metadata":{"id":"wQ6cy3KnF4s_"},"source":["def preprocessing_step(data, model, preprocessing, augmentation):\n","  tokenizer = Tokenizer()\n","  tokenizer.fit_on_texts(data['tweet'].values)\n","  \n","  file_name = 'tokenizer_'\n","\n","  if preprocessing:\n","    if augmentation:\n","      file_name = file_name + model + '_preprocessing_augmentantion.pickle'\n","    else:\n","      file_name = file_name + model + '_preprocessing.pickle'\n","  else:\n","    if augmentation:\n","      file_name = file_name + model + '_augmentantion.pickle'\n","    else:\n","      file_name = file_name + model + '.pickle'\n","\n","  with open('Tokenizer/'+file_name, 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","  data['tokenized'] = tokenizer.texts_to_sequences(data['tweet'].values)\n","\n","  vocab_size = len(tokenizer.word_index) + 1\n","\n","  X = pad_sequences(sequences = data['tokenized'],\n","                  maxlen = length_size,\n","                  padding = 'post')\n","\n","  y = data['label']\n","\n","  X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.15, random_state=23)\n","\n","  return vocab_size, tokenizer, X_train, X_validation, y_train, y_validation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Raq3KN353mxO"},"source":["## Predict"]},{"cell_type":"code","metadata":{"id":"dSZq35fD3lTY"},"source":["def predict(tokenizer, model, prep, augmentation, nome):\n","\n","  test = pd.read_csv('Data/test.csv')\n","\n","  if prep:\n","    test = preprocessing(test)\n","    test['tokenized'] = tokenizer.texts_to_sequences(test['cleaned_tweet'].values)\n","  else:\n","    test['tokenized'] = tokenizer.texts_to_sequences(test['tweet'].values)\n","\n","  X_test = pad_sequences(sequences = test['tokenized'],\n","                         maxlen = length_size,\n","                         padding = 'post')\n","\n","  predicted = (model.predict(X_test) > 0.5).astype(\"int32\")\n","  prediction = pd.DataFrame()\n","  prediction['id'] = test['id']\n","  prediction['label'] = predicted\n","\n","  if prep:\n","\n","    if augmentation:\n","      prediction.to_csv('Submission/' + nome + '_preprocessing_augmentation.csv', index=False)\n","    else:\n","      prediction.to_csv('Submission/' + nome + '_preprocessing.csv', index=False)\n","\n","  else:\n","\n","    if augmentation:\n","      prediction.to_csv('Submission/' + nome + '_augmentation.csv', index=False)\n","    else:\n","      prediction.to_csv('Submission/' + nome + '.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KmfpDUot3oyv"},"source":["## Save Models"]},{"cell_type":"code","metadata":{"id":"_ou2_2Zq3o7f"},"source":["def save_model(modelo, units):\n","  \n","  file_name = 'model_'\n","  file_name = file_name + nome_modelo + '_preprocessing_augmentantion'\n","  \n","  modelo.save('Model/' + file_name)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PWYmZv062f_D"},"source":["## Save embeddings"]},{"cell_type":"code","metadata":{"id":"xtMYplLE2gPS"},"source":["def save_embedding(modelo, tokenizer, nome_modelo, preprocessing, augmentation):\n","  embeddings = modelo.get_layer('embedding').get_weights()[0]\n","  w2v_my = {}\n","\n","  for word, index in tokenizer.word_index.items():\n","      w2v_my[word] = embeddings[index]\n","  \n","  file_name = 'embedding_'\n","\n","  if preprocessing:\n","    if augmentation:\n","      file_name = file_name + nome_modelo + '_preprocessing_augmentantion.pickle'\n","    else:\n","      file_name = file_name + nome_modelo + '_preprocessing.pickle'\n","  else:\n","    if augmentation:\n","      file_name = file_name + nome_modelo + '_augmentantion.pickle'\n","    else:\n","      file_name = file_name + nome_modelo + '.pickle'\n","\n","  with open('Model/' + file_name + '.h5', 'wb') as handle:\n","    pickle.dump(w2v_my, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xn-bmRXK8fwV"},"source":["## CNN-rand"]},{"cell_type":"code","metadata":{"id":"9A-7AYJW8fMI"},"source":["def cnn_rand(vocab_size, tokenizer, X_train, X_validation, y_train, y_validation, preprocessing, augmentation):\n","\n","  #model input\n","  input = Input(shape=(length_size, ))\n","\n","  #embedding layer\n","  embedding = Embedding(input_dim=vocab_size,\n","                        output_dim=embedding_dim,\n","                        input_length=length_size,\n","                        name='embedding')(input)\n","\n","  reshape = Reshape((length_size, embedding_dim, 1))(embedding)\n","\n","  #convolution layer\n","  convs = []\n","  for size in kernel_size:\n","    conv = Convolution2D(filters=filters,\n","                        kernel_size=(size, embedding_dim),\n","                        activation=non_linearity_function,\n","                        padding='same',\n","                        kernel_regularizer=l2(l2_constraint))(reshape)\n","    \n","    pool = MaxPooling2D(strides=(1, 1),\n","                        pool_size=(2, 1),\n","                        padding='valid')(conv)\n","    convs.append(pool)\n","\n","  #concatenate convs layers\n","  concatenated = Concatenate(axis=1)(convs)\n","\n","  #flatten layer\n","  flatten = Flatten()(concatenated)\n","\n","  #droupout layer\n","  dropout = Dropout(0.5)(flatten)\n","\n","  #output layer\n","  output = Dense(units=1, activation='sigmoid')(dropout)\n","\n","  model_random = Model(inputs=input, outputs=output)\n","  model_random.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","  #model_random.summary()\n","\n","  history_random = model_random.fit(X_train, \n","                                    y_train,\n","                                    batch_size=batch_size,\n","                                    epochs=epochs,\n","                                    validation_data=(X_validation, y_validation))\n","  \n","  save_embedding(model_random, tokenizer, 'CNN-rand', preprocessing, augmentation)\n","\n","  save_model(model_random, 'CNN-rand', preprocessing, augmentation)\n","\n","  predicted_validation = (model_random.predict(X_validation) > 0.5).astype(\"int32\")\n","  score = f1_score(y_validation, predicted_validation, average='weighted')\n","  score = round(score, 4)\n","\n","  predict(tokenizer, model_random, preprocessing, augmentation, 'CNN-rand')\n","  \n","  return model_random, history_random, score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CI9jb9a40sWZ"},"source":["## CNN-static"]},{"cell_type":"code","metadata":{"id":"JDx6XooT0ulS"},"source":["def cnn_static(vocab_size, tokenizer, X_train, X_validation, y_train, y_validation, preprocessing, augmentation):\n","\n","  #model input\n","  input = Input(shape=(length_size, ))\n","\n","  #embedding layer\n","  if preprocessing:\n","    embedding = Embedding(input_dim=vocab_size,\n","                          output_dim=embedding_dim,\n","                          input_length=length_size,\n","                          weights=[word2vec_preprocessing_embedding],\n","                          trainable=False,\n","                          name='embedding')(input)\n","  else:\n","    embedding = Embedding(input_dim=vocab_size,\n","                          output_dim=embedding_dim,\n","                          input_length=length_size,\n","                          weights=[word2vec_embedding],\n","                          trainable=False,\n","                          name='embedding')(input)\n","\n","  reshape = Reshape((length_size, embedding_dim, 1))(embedding)\n","\n","  #convolution layer\n","  convs = []\n","  for size in kernel_size:\n","    conv = Convolution2D(filters=filters,\n","                        kernel_size=(size, embedding_dim),\n","                        activation=non_linearity_function,\n","                        kernel_regularizer=l2(l2_constraint))(reshape)\n","    \n","    pool = MaxPooling2D(strides=(1, 1),\n","                        pool_size=(2, 1),\n","                        padding='valid')(conv)\n","    convs.append(pool)\n","\n","  #concatenate convs layers\n","  concatenated = Concatenate(axis=1)(convs)\n","\n","  #flatten layer\n","  flatten = Flatten()(concatenated)\n","\n","  #droupout layer\n","  dropout = Dropout(0.5)(flatten)\n","\n","  #output layer\n","  output = Dense(units=1, activation='sigmoid')(dropout)\n","\n","  model_static = Model(inputs=input, outputs=output)\n","  model_static.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","  #model_static.summary()\n","\n","  history_static = model_static.fit(X_train, \n","                                    y_train,\n","                                    batch_size=batch_size,\n","                                    epochs=epochs,\n","                                    validation_data=(X_validation, y_validation))\n","  \n","  save_embedding(model_static, tokenizer, 'CNN-static', preprocessing, augmentation)\n","\n","  save_model(model_static, 'CNN-static', preprocessing, augmentation)\n","\n","  predicted_validation = (model_static.predict(X_validation) > 0.5).astype(\"int32\")\n","  score = f1_score(y_validation, predicted_validation, average='weighted')\n","  score = round(score, 4)\n","\n","  predict(tokenizer, model_static, preprocessing, augmentation, 'CNN-static')\n","  \n","  return model_static, history_static, score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3f-PZSIBoaS"},"source":["## CNN non-static"]},{"cell_type":"code","metadata":{"id":"KC_AHFhvBq7h"},"source":["def cnn_non_static(vocab_size, tokenizer, X_train, X_validation, y_train, y_validation, preprocessing, augmentation):\n","\n","  #model input\n","  input = Input(shape=(length_size, ))\n","\n","  #embedding layer\n","  if preprocessing:\n","    embedding = Embedding(input_dim=vocab_size,\n","                          output_dim=embedding_dim,\n","                          input_length=length_size,\n","                          weights=[word2vec_preprocessing_embedding],\n","                          trainable=True,\n","                          name='embedding')(input)\n","  else:\n","    embedding = Embedding(input_dim=vocab_size,\n","                          output_dim=embedding_dim,\n","                          input_length=length_size,\n","                          weights=[word2vec_embedding],\n","                          trainable=True,\n","                          name='embedding')(input)\n","\n","  reshape = Reshape((length_size, embedding_dim, 1))(embedding)\n","\n","  #convolution layer\n","  convs = []\n","  for size in kernel_size:\n","    conv = Convolution2D(filters=filters,\n","                        kernel_size=(size, embedding_dim),\n","                        activation=non_linearity_function,\n","                        kernel_regularizer=l2(l2_constraint))(reshape)\n","    \n","    pool = MaxPooling2D(strides=(1, 1),\n","                        pool_size=(2, 1),\n","                        padding='valid')(conv)\n","    convs.append(pool)\n","\n","  #concatenate convs layers\n","  concatenated = Concatenate(axis=1)(convs)\n","\n","  #flatten layer\n","  flatten = Flatten()(concatenated)\n","\n","  #droupout layer\n","  dropout = Dropout(0.5)(flatten)\n","\n","  #output layer\n","  output = Dense(units=1, activation='sigmoid')(dropout)\n","\n","  model_non_static = Model(inputs=input, outputs=output)\n","  model_non_static.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","  #model_non_static.summary()\n","\n","  history_non_static = model_non_static.fit(X_train, \n","                                            y_train,\n","                                            batch_size=batch_size,\n","                                            epochs=epochs,\n","                                            validation_data=(X_validation, y_validation))\n","  \n","  save_embedding(model_non_static, tokenizer, 'CNN-non-static', preprocessing, augmentation)\n","\n","  save_model(model_non_static, 'CNN-non-static', preprocessing, augmentation)\n","\n","  predicted_validation = (model_non_static.predict(X_validation) > 0.5).astype(\"int32\")\n","  score = f1_score(y_validation, predicted_validation, average='weighted')\n","  score = round(score, 4)\n","\n","  predict(tokenizer, model_non_static, preprocessing, augmentation, 'CNN-non-static')\n","\n","  return model_non_static, history_non_static, score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cg6enS3gDizi"},"source":["## Main"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"xDxJkScXDlFI","outputId":"3ec85db5-faee-4e1c-a6fb-b7a996cddc87"},"source":["use_augmentation = [True]\n","use_preprocessing = [True]\n","models_used = []\n","preprocessing_used = []\n","augmentation_used = []\n","scores_validation = []\n","models = ['rand']\n","\n","for model in models:\n","\n","  for aug in use_augmentation:\n","\n","    for prep in use_preprocessing:\n","\n","      if aug and prep:\n","        vocab_size, tokenizer, X_train, X_validation, y_train, y_validation = preprocessing_step(data_preprocessing_augmentation,  model, prep, aug)\n","      elif aug and not prep:\n","        vocab_size, tokenizer, X_train, X_validation, y_train, y_validation = preprocessing_step(data_augmentation,  model, prep, aug)\n","      elif prep and not aug:\n","        vocab_size, tokenizer, X_train, X_validation, y_train, y_validation = preprocessing_step(preprocessed_data,  model, prep, aug)\n","      else:\n","        vocab_size, tokenizer, X_train, X_validation, y_train, y_validation = preprocessing_step(normal_data, model, prep, aug) \n","\n","      print('Modelo: {}\\nPré-processamento: {}\\nBalanceamento: {}\\n'.format(model, prep, aug))\n","\n","      if model == 'rand':\n","        m, history, validation_score = cnn_rand(vocab_size, tokenizer, X_train, X_validation, y_train, y_validation, prep, aug)\n","      elif model == 'static':\n","        m, history, validation_score = cnn_static(vocab_size, tokenizer, X_train, X_validation, y_train, y_validation, prep, aug)\n","      elif model == 'non_static':\n","        m, history, validation_score = cnn_non_static(vocab_size, tokenizer, X_train, X_validation, y_train, y_validation, prep, aug)\n","      else:\n","        print('Modelo não encontrado')\n","        break\n","        \n","      print('Modelo finalizado!\\n')\n","\n","      models_used.append(model)\n","      preprocessing_used.append(prep)\n","      augmentation_used.append(aug)\n","      scores_validation.append(validation_score)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Modelo: rand\n","Pré-processamento: True\n","Balanceamento: True\n","\n","Epoch 1/10\n","  1/468 [..............................] - ETA: 11:08:53 - loss: 18.5267 - accuracy: 0.4700"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-d4455f62d1fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rand'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_rand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'static'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_static\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-6ae307bc78b7>\u001b[0m in \u001b[0;36mcnn_rand\u001b[0;34m(vocab_size, tokenizer, X_train, X_validation, y_train, y_validation, preprocessing, augmentation)\u001b[0m\n\u001b[1;32m     46\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                                     validation_data=(X_validation, y_validation))\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0msave_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CNN-rand'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"4PJDAY2JDlIm"},"source":["results = pd.DataFrame()\n","results['modelo'] = models_used\n","results['pré_processamento'] = preprocessing_used\n","results['balanceamento'] = augmentation_used\n","results['score_validação'] = scores_validation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"C0wXkrN4rMUd"},"source":["results.to_csv('resultados_textCNN2D.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"54rzghrL3jNS"},"source":["!zip -r /content/model.zip /content/Model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"Mdhry_XBybwY"},"source":["!zip -r /content/submission.zip /content/Submission/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"nHyl8viViEEp"},"source":["!zip -r /content/tokenizer.zip /content/Tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"WxHcIEk8ffXQ"},"source":["while True:\n","  pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u8d45Vi10XSX"},"source":["## Referências"]},{"cell_type":"markdown","metadata":{"id":"N3-LaAxV0WJw"},"source":["https://www.kaggle.com/hamishdickson/cnn-for-sentence-classification-by-yoon-kim\n","\n","https://github.com/pinkeshbadjatiya/twitter-hatespeech/blob/master/cnn.py\n","\n","https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/sentiment_cnn.py\n","\n","https://github.com/satya-thirumani/Python/blob/master/Sentiment%20Analysis/AV_practice_problem_Twitter_Sentiment_Analysis.ipynb\n","\n","https://github.com/yoonkim/CNN_sentence/blob/23e0e1f7355705bb083043fda05c031b15acb38c/conv_net_classes.py#L340\n","\n","https://github.com/Jverma/cnn-text-classification-keras/blob/master/text_cnn.py\n","\n","https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n","\n","https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py"]}]}