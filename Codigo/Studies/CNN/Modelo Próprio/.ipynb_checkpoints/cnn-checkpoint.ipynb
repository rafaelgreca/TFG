{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-mHbP_8M8Abx",
   "metadata": {
    "executionInfo": {
     "elapsed": 25669,
     "status": "ok",
     "timestamp": 1628894195389,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "-mHbP_8M8Abx"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas\n",
    "!pip install tensorflow\n",
    "!pip install -U scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install numpy\n",
    "!pip install nltk\n",
    "!pip install unidecode\n",
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTgtwqwT8XFa",
   "metadata": {
    "id": "kTgtwqwT8XFa"
   },
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MUbg5wzUtCwo",
   "metadata": {
    "id": "MUbg5wzUtCwo"
   },
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intelligent-input",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2481,
     "status": "ok",
     "timestamp": 1628894197851,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "intelligent-input",
    "outputId": "ff33ba05-2dcf-40b9-ec3f-b438235fd6b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 17:48:23.173126: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-21 17:48:23.173178: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package stopwords to /home/greca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten, Dense, Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "es = EarlyStopping(patience = 3)\n",
    "np.random.seed(23)\n",
    "sns.set_style('dark')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3uuhfExasoOR",
   "metadata": {
    "id": "3uuhfExasoOR"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Epj-NAcWs_CF",
   "metadata": {
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1628894197859,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "Epj-NAcWs_CF"
   },
   "outputs": [],
   "source": [
    "def remove_username(text):\n",
    "  text = re.sub(r'\\@[^\\s]+', ' ', text)\n",
    "  return text\n",
    "\n",
    "def remove_newline(text):\n",
    "  text = text.replace('\\n', ' ')\n",
    "  return text\n",
    "\n",
    "def only_letters(text):\n",
    "  text = re.sub(r'[^A-Za-z]+', ' ', text)\n",
    "  return text\n",
    "\n",
    "def remove_link(text):\n",
    "  text = re.sub(r'www\\.?[^\\s]+', ' ', text)\n",
    "  return text\n",
    "\n",
    "def remove_hyperlink(text):\n",
    "  text = re.sub(r'\\<.?\\>', ' ', text)\n",
    "  return text\n",
    "\n",
    "def remove_accent(text):\n",
    "  text = unidecode.unidecode(text)\n",
    "  return text\n",
    "\n",
    "def adjustment_text(text):\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  text = text.strip()\n",
    "  return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  text = [word for word in text.split() if word not in sw]\n",
    "  text = ' '.join(text)\n",
    "  return text\n",
    "\n",
    "def remove_spam(text):\n",
    "  text = re.sub(r'\\&amp', ' ', text)\n",
    "  text = re.sub(r'\\&lt', ' ', text)\n",
    "  text = re.sub(r'\\&gt', ' ', text)\n",
    "  text = re.sub(r'\\#follow|\\#followme|\\#like|\\#f4f|\\#photooftheday', ' ', text)\n",
    "  return text\n",
    "\n",
    "def remove_slangs(text):\n",
    "  text = re.sub(r' b4 ', ' before ', text)\n",
    "  text = re.sub(r' 2b ', ' to be ', text)\n",
    "  text = re.sub(r' 2morrow ', ' tomorrow ', text)\n",
    "  text = re.sub(r' rn ', ' right now ', text)\n",
    "  text = re.sub(r' brb ', ' be right back ', text)\n",
    "  text = re.sub(r' mb ', ' my bad ', text)\n",
    "  text = re.sub(r' luv ', ' love ', text)\n",
    "  text = re.sub(r' b ', ' be ', text)\n",
    "  text = re.sub(r' r ', ' are ', text)\n",
    "  text = re.sub(r' u ', ' you ', text)\n",
    "  text = re.sub(r' y ', ' why ', text)\n",
    "  text = re.sub(r' ur ', ' your ', text)\n",
    "  text = re.sub(r' hbd ', ' happy birthday ', text)\n",
    "  text = re.sub(r' bday ', ' birthday ', text)\n",
    "  text = re.sub(r' bihday ', ' birthday ', text)\n",
    "  text = re.sub(r' omg ', ' oh my god ', text)\n",
    "  text = re.sub(r' lol ', ' laughing out loud ', text)\n",
    "  return text\n",
    "\n",
    "def remove_abbreviations(text):\n",
    "  text = re.sub(r\" can\\'t \", \" can not \", text)\n",
    "  text = re.sub(r\" i\\'m \", \" i am \", text)\n",
    "  text = re.sub(r\" i\\'ll \", \" i will \", text)\n",
    "  text = re.sub(r\" i\\'d \", \" i would \", text)\n",
    "  text = re.sub(r\" i\\'ve \", \" i have \", text)\n",
    "  text = re.sub(r\" ain\\'t \", \" am not \", text)\n",
    "  text = re.sub(r\" haven\\'t \", \" have not \", text)\n",
    "  text = re.sub(r\" hasn\\'t \", \" has not \", text)\n",
    "  text = re.sub(r\" can\\'t \", \" can not \", text)\n",
    "  text = re.sub(r\" won\\'t \", \" will not \", text)\n",
    "  text = re.sub(r\" you\\'re \", \" you are \", text)\n",
    "  text = re.sub(r\" we\\'re \", \" we are \", text)\n",
    "  text = re.sub(r\" they\\'re \", \" they are \", text)\n",
    "  text = re.sub(r\" he\\'s \", \" he is \", text)\n",
    "  text = re.sub(r\" she\\'s \", \" she is \", text)\n",
    "  text = re.sub(r\" it\\'s \", \" it is \", text)\n",
    "  text = re.sub(r\" don\\'t \", \" do not \", text)\n",
    "  text = re.sub(r\" doesn\\'t \", \" does not \", text)\n",
    "  text = re.sub(r\" wouldn\\'t \", \" would not \", text)\n",
    "  text = re.sub(r\" couldn\\'t \", \" could not \", text)\n",
    "  text = re.sub(r\" shouldn\\'t \", \" should not \", text)\n",
    "  text = re.sub(r\" no\\-one \", \" noone \", text)\n",
    "  return text\n",
    "\n",
    "def remove_one_len_word(text):\n",
    "  text = re.sub(r'\\b[a-z]\\b', ' ', text)\n",
    "  return text\n",
    "\n",
    "def preprocessing(data):\n",
    "  data['cleaned_tweet'] = data['tweet'].apply(str)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(lambda x: x.lower())\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_newline)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_hyperlink)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_spam)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_link)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_username)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_accent)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_abbreviations)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(only_letters)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_slangs)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_stopwords)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(remove_one_len_word)\n",
    "  data['cleaned_tweet'] = data['cleaned_tweet'].apply(adjustment_text)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b43940",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"#model   i love u take with u all the time in urð±!!! ðððð",
    "ð¦ð¦ð¦  \",\n",
    "        \"@user #cnn calls #michigan middle school 'build the wall' chant '' #tcot\",  \n",
    "        \"@user i'll always hope that one day i'll get to hug you, but i don't think that it's gonna happen anytime soon... \",\n",
    "        \"i am thankful for good friends. #thankful #positive\",\n",
    "ð©the white establishment can't have blk folx running around loving themselves and promoting our greatness  \",\n",
    "words r free, it's how u use em that can cost you! #verbal #abuse   #hu #love  #adult #teen @user \"]\n",
    "\n",
    "textos = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U84N41FtsxSJ",
   "metadata": {
    "id": "U84N41FtsxSJ"
   },
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-parade",
   "metadata": {
    "id": "laden-parade"
   },
   "source": [
    "## Modelo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rYNseLx7-1NN",
   "metadata": {
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1628894197862,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "rYNseLx7-1NN"
   },
   "outputs": [],
   "source": [
    "def modelo1(X_train,\n",
    "            X_validation,\n",
    "            y_train,\n",
    "            y_validation,\n",
    "            length_size,\n",
    "            embedding_dim,\n",
    "            vocab_size,\n",
    "            preprocessing):\n",
    "  model1 = Sequential()\n",
    "\n",
    "  model1.add(Embedding(input_dim = vocab_size,\n",
    "                        output_dim = embedding_dim,\n",
    "                        input_length = length_size))\n",
    "    \n",
    "  model1.add(Conv1D(filters = 32,\n",
    "                   kernel_size = 3,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model1.add(MaxPooling1D())\n",
    "  model1.add(Flatten())\n",
    "  model1.add(Dropout(rate = 0.2))\n",
    "  model1.add(Dense(units = 1,\n",
    "                  activation = 'sigmoid'))\n",
    "  \n",
    "  #model.summary()\n",
    "  model1.compile(optimizer = 'adam',\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "  history_model1 = model1.fit(x = X_train,\n",
    "                            y = y_train,\n",
    "                            validation_data = (X_validation, y_validation),\n",
    "                            batch_size = 100,\n",
    "                            epochs = 20,\n",
    "                            callbacks = [es],\n",
    "                            verbose = 0)\n",
    "  \n",
    "  predicted = (model1.predict(X_validation) > 0.5).astype(\"int32\")\n",
    "  score1 = f1_score(predicted, y_validation, average='weighted')\n",
    "  return model1, score1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r0rq3djpCXxH",
   "metadata": {
    "id": "r0rq3djpCXxH"
   },
   "source": [
    "## Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5XTt6U4_dYP",
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1628894197867,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "a5XTt6U4_dYP"
   },
   "outputs": [],
   "source": [
    "def modelo2(X_train, \n",
    "            X_validation, \n",
    "            y_train, \n",
    "            y_validation, \n",
    "            length_size, \n",
    "            embedding_dim, \n",
    "            vocab_size,\n",
    "            preprocess):\n",
    "  model2 = Sequential()\n",
    "\n",
    "  model2.add(Embedding(input_dim = vocab_size,\n",
    "                        output_dim = embedding_dim,\n",
    "                        input_length = length_size))\n",
    "    \n",
    "  model2.add(Conv1D(filters = 32,\n",
    "                   kernel_size = 3,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model2.add(MaxPooling1D())\n",
    "  model2.add(Conv1D(filters = 64,\n",
    "                   kernel_size = 5,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model2.add(MaxPooling1D())\n",
    "  model2.add(Flatten())\n",
    "  model2.add(Dropout(rate = 0.2))\n",
    "  model2.add(Dense(units = 1,\n",
    "                  activation = 'sigmoid'))\n",
    "  \n",
    "  #model.summary()\n",
    "  model2.compile(optimizer = 'adam',\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "  history_model2 = model2.fit(x = X_train,\n",
    "                            y = y_train,\n",
    "                            validation_data = (X_validation, y_validation),\n",
    "                            batch_size = 100,\n",
    "                            epochs = 20,\n",
    "                            callbacks = [es],\n",
    "                            verbose = 0)\n",
    "  \n",
    "  predicted = (model2.predict(X_validation) > 0.5).astype(\"int32\")\n",
    "  score2 = f1_score(predicted, y_validation, average='weighted')\n",
    "  return model2, score2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ipy94TLMcs",
   "metadata": {
    "id": "59ipy94TLMcs"
   },
   "source": [
    "## Modelo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MqMIGrzdF1SK",
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1628894197871,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "MqMIGrzdF1SK"
   },
   "outputs": [],
   "source": [
    "def modelo3(X_train, \n",
    "            X_validation, \n",
    "            y_train, \n",
    "            y_validation, \n",
    "            length_size, \n",
    "            embedding_dim, \n",
    "            vocab_size,\n",
    "            preprocess):\n",
    "  model3 = Sequential()\n",
    "  \n",
    "  model3.add(Embedding(input_dim = vocab_size,\n",
    "                        output_dim = embedding_dim,\n",
    "                        input_length = length_size))\n",
    "    \n",
    "  model3.add(Conv1D(filters = 32,\n",
    "                   kernel_size = 3,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model3.add(MaxPooling1D())\n",
    "  model3.add(Conv1D(filters = 64,\n",
    "                   kernel_size = 5,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model3.add(MaxPooling1D())\n",
    "  model3.add(Conv1D(filters = 128,\n",
    "                   kernel_size = 7,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model3.add(MaxPooling1D())\n",
    "  model3.add(Flatten())\n",
    "  model3.add(Dropout(rate = 0.2))\n",
    "  model3.add(Dense(units = 1,\n",
    "                  activation = 'sigmoid'))\n",
    "  \n",
    "  #model.summary()\n",
    "  model3.compile(optimizer = 'adam',\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "  history_model3 = model3.fit(x = X_train,\n",
    "                            y = y_train,\n",
    "                            validation_data = (X_validation, y_validation),\n",
    "                            batch_size = 100,\n",
    "                            epochs = 20,\n",
    "                            callbacks = [es],\n",
    "                            verbose = 0)\n",
    "  \n",
    "  predicted = (model3.predict(X_validation) > 0.5).astype(\"int32\")\n",
    "  score3 = f1_score(predicted, y_validation, average='weighted')\n",
    "  return model3, score3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8r0S-aYwLrSS",
   "metadata": {
    "id": "8r0S-aYwLrSS"
   },
   "source": [
    "## Modelo 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wxtWuv4EAACr",
   "metadata": {
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1628894198372,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "wxtWuv4EAACr"
   },
   "outputs": [],
   "source": [
    "def modelo4(X_train,\n",
    "            X_validation,\n",
    "            y_train,\n",
    "            y_validation,\n",
    "            length_size,\n",
    "            embedding_dim,\n",
    "            vocab_size,\n",
    "            preprocess):\n",
    "  model4 = Sequential()\n",
    "  \n",
    "  model4.add(Embedding(input_dim = vocab_size,\n",
    "                        output_dim = embedding_dim,\n",
    "                        input_length = length_size))\n",
    "    \n",
    "  model4.add(Conv1D(filters = 32,\n",
    "                   kernel_size = 3,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model4.add(MaxPooling1D())\n",
    "  model4.add(Conv1D(filters = 64,\n",
    "                   kernel_size = 5,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model4.add(MaxPooling1D())\n",
    "  model4.add(Flatten())\n",
    "  model4.add(Dropout(rate = 0.2))\n",
    "  model4.add(Dense(units = 64,\n",
    "                  activation = 'relu'))\n",
    "  model4.add(Dense(units = 1,\n",
    "                  activation = 'sigmoid'))\n",
    "  \n",
    "  #model.summary()\n",
    "  model4.compile(optimizer = 'adam',\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "  history_model4 = model4.fit(x = X_train,\n",
    "                            y = y_train,\n",
    "                            validation_data = (X_validation, y_validation),\n",
    "                            batch_size = 100,\n",
    "                            epochs = 20,\n",
    "                            callbacks = [es],\n",
    "                            verbose = 0)\n",
    "  \n",
    "  predicted = (model4.predict(X_validation) > 0.5).astype(\"int32\")\n",
    "  score4 = f1_score(predicted, y_validation, average='weighted')\n",
    "  return model4, score4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XBz-y8sEFvzU",
   "metadata": {
    "id": "XBz-y8sEFvzU"
   },
   "source": [
    "## Modelo 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zKAb-9jC_ut1",
   "metadata": {
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1628894198376,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "zKAb-9jC_ut1"
   },
   "outputs": [],
   "source": [
    "def modelo5(X_train, \n",
    "            X_validation, \n",
    "            y_train, \n",
    "            y_validation, \n",
    "            length_size, \n",
    "            embedding_dim, \n",
    "            vocab_size, \n",
    "            preprocess):\n",
    "  model5 = Sequential()\n",
    "  \n",
    "  model5.add(Embedding(input_dim = vocab_size,\n",
    "                        output_dim = embedding_dim,\n",
    "                        input_length = length_size))\n",
    "    \n",
    "  model5.add(Conv1D(filters = 32,\n",
    "                   kernel_size = 3,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model5.add(MaxPooling1D())\n",
    "  model5.add(Flatten())\n",
    "  model5.add(Dropout(rate = 0.2))\n",
    "  model5.add(Dense(units = 64,\n",
    "                  activation = 'relu'))\n",
    "  model5.add(Dense(units = 1,\n",
    "                  activation = 'sigmoid'))\n",
    "  \n",
    "  #model.summary()\n",
    "  model5.compile(optimizer = 'adam',\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "  history_model5 = model5.fit(x = X_train,\n",
    "                            y = y_train,\n",
    "                            validation_data = (X_validation, y_validation),\n",
    "                            batch_size = 100,\n",
    "                            epochs = 20,\n",
    "                            callbacks = [es],\n",
    "                            verbose = 0)\n",
    "  \n",
    "  predicted = (model5.predict(X_validation) > 0.5).astype(\"int32\")\n",
    "  score5 = f1_score(predicted, y_validation, average='weighted')\n",
    "  return model5, score5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jnO19TNnFzNV",
   "metadata": {
    "id": "jnO19TNnFzNV"
   },
   "source": [
    "## Modelo 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ci7HBImnF12-",
   "metadata": {
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1628894198378,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "Ci7HBImnF12-"
   },
   "outputs": [],
   "source": [
    "def modelo6(X_train, \n",
    "            X_validation,\n",
    "            y_train, \n",
    "            y_validation, \n",
    "            length_size, \n",
    "            embedding_dim, \n",
    "            vocab_size, \n",
    "            preprocess):\n",
    "  model6 = Sequential()\n",
    "  \n",
    "  model6.add(Embedding(input_dim = vocab_size,\n",
    "                        output_dim = embedding_dim,\n",
    "                        input_length = length_size))\n",
    "    \n",
    "  model6.add(Conv1D(filters = 32,\n",
    "                   kernel_size = 3,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model6.add(MaxPooling1D())\n",
    "  model6.add(Conv1D(filters = 64,\n",
    "                   kernel_size = 5,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model6.add(MaxPooling1D())\n",
    "  model6.add(Conv1D(filters = 128,\n",
    "                   kernel_size = 7,\n",
    "                   padding = 'same',\n",
    "                   activation = 'relu'))\n",
    "  model6.add(MaxPooling1D())\n",
    "  model6.add(Flatten())\n",
    "  model6.add(Dropout(rate = 0.2))\n",
    "  model6.add(Dense(units = 64,\n",
    "                  activation = 'relu'))\n",
    "  model6.add(Dense(units = 1,\n",
    "                  activation = 'sigmoid'))\n",
    "  \n",
    "  #model.summary()\n",
    "  model6.compile(optimizer = 'adam',\n",
    "                loss = 'binary_crossentropy',\n",
    "                metrics = ['accuracy'])\n",
    "  \n",
    "  history_model6 = model6.fit(x = X_train,\n",
    "                            y = y_train,\n",
    "                            validation_data = (X_validation, y_validation),\n",
    "                            batch_size = 100,\n",
    "                            epochs = 20,\n",
    "                            callbacks = [es],\n",
    "                            verbose = 0)\n",
    "  \n",
    "  predicted = (model6.predict(X_validation) > 0.5).astype(\"int32\")\n",
    "  score6 = f1_score(predicted, y_validation, average='weighted')\n",
    "  return model6, score6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LZUD_tmjqIw3",
   "metadata": {
    "id": "LZUD_tmjqIw3"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ciDWb9FlAcxa",
   "metadata": {
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1628894198381,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "ciDWb9FlAcxa"
   },
   "outputs": [],
   "source": [
    "def predict(model, nome, use_preprocessing, tokenizer, augmentation):\n",
    "  test = pd.read_csv('Data/test.csv')\n",
    "\n",
    "  if use_preprocessing:\n",
    "    test = preprocessing(test)\n",
    "    test['tokenized'] = tokenizer.texts_to_sequences(test['cleaned_tweet'])\n",
    "  else:\n",
    "    test['tokenized'] = tokenizer.texts_to_sequences(test['tweet'])\n",
    "\n",
    "  X_test = pad_sequences(sequences = test['tokenized'],\n",
    "                         maxlen = length_size,\n",
    "                         padding = 'post')\n",
    "\n",
    "  predicted = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "  prediction = pd.DataFrame()\n",
    "  prediction['id'] = test['id']\n",
    "  prediction['label'] = predicted\n",
    "\n",
    "  if use_preprocessing:\n",
    "\n",
    "    if augmentation:\n",
    "      prediction.to_csv('Submission/ros' + nome + '_preprocessamento_balanceamento.csv', index=False)\n",
    "    else:\n",
    "      prediction.to_csv('Submission/ros' + nome + '_preprocessamento.csv', index=False)\n",
    "  else:\n",
    "\n",
    "    if augmentation:\n",
    "      prediction.to_csv('Submission/ros' + nome + '_balanceamento.csv', index=False)\n",
    "    else:\n",
    "      prediction.to_csv('Submission/ros' + nome + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_h6GOY30Bm5N",
   "metadata": {
    "id": "_h6GOY30Bm5N"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tmFRaTT4WFtK",
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1628894198384,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "tmFRaTT4WFtK"
   },
   "outputs": [],
   "source": [
    "def preprocessing_step(use_preprocessing, test_size, data):\n",
    "  tokenizer = Tokenizer()\n",
    "\n",
    "  if use_preprocessing:\n",
    "    tokenizer.fit_on_texts(data['cleaned_tweet'])\n",
    "    data['tokenized'] = tokenizer.texts_to_sequences(data['cleaned_tweet'])\n",
    "  \n",
    "  else:\n",
    "    tokenizer.fit_on_texts(data['tweet'])\n",
    "    data['tokenized'] = tokenizer.texts_to_sequences(data['tweet'])\n",
    "\n",
    "  vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "  X = pad_sequences(sequences = data['tokenized'],\n",
    "                  maxlen = length_size,\n",
    "                  padding = 'post')\n",
    "\n",
    "  y = data['label']\n",
    "\n",
    "  X_train, X_validation, y_train, y_validation = train_test_split(X,\n",
    "                                                                  y, \n",
    "                                                                  test_size = test_size,\n",
    "                                                                  random_state = 23)\n",
    "  \n",
    "  return X_train, X_validation, y_train, y_validation, vocab_size, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FWYSWdGNBpAI",
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1628894198386,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "FWYSWdGNBpAI"
   },
   "outputs": [],
   "source": [
    "test_size = 0.15\n",
    "length_size = 30\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-boards",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1628894198394,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "outside-boards",
    "outputId": "92052610-baec-48c2-e326-da3939216b3c"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/train.csv')\n",
    "data = data.drop(columns=['id'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y1iBXFt325sx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 2873,
     "status": "ok",
     "timestamp": 1628894201226,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "Y1iBXFt325sx",
    "outputId": "848be287-19cb-44e2-d8b0-5d9e2d0b9a0a"
   },
   "outputs": [],
   "source": [
    "data = preprocessing(data)\n",
    "data = data.dropna()\n",
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vBB0Hsb03DCB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1628894201228,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "vBB0Hsb03DCB",
    "outputId": "b6aba6fa-357a-4c76-9405-5a93b7f6bc68"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5juBuYgc96Ll",
   "metadata": {
    "id": "5juBuYgc96Ll"
   },
   "source": [
    "data_augmentation_prep = pd.read_csv('Data/train_preprocessing_augmented.csv')\n",
    "data_augmentation_prep['cleaned_tweet'] = data_augmentation_prep['cleaned_tweet'].apply(str)\n",
    "data_augmentation_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uVMp3D_Alqjw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1628894201230,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "uVMp3D_Alqjw",
    "outputId": "d342397e-7d86-4117-8275-ba2f6a54dd49"
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=23, sampling_strategy='minority')\n",
    "X_resampled, y_resampled = ros.fit_resample(data[['cleaned_tweet']], data['label'])\n",
    "data_augmentation_prep = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "data_augmentation_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kXcX8mY5-LuB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1628894201234,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "kXcX8mY5-LuB",
    "outputId": "ccc87f3e-80df-42bd-d3a2-b610204bb901"
   },
   "outputs": [],
   "source": [
    "data_augmentation_prep.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C9RSuJm0-FTU",
   "metadata": {
    "id": "C9RSuJm0-FTU"
   },
   "source": [
    "data_augmentation_noprep = pd.read_csv('Data/train_nopreprocessing_augmented.csv')\n",
    "data_augmentation_noprep['tweet'] = data_augmentation_noprep['tweet'].apply(str)\n",
    "data_augmentation_noprep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RxEG1BUWlr67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1628894201236,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "RxEG1BUWlr67",
    "outputId": "7ce1f4b9-050b-4372-9393-92e8e62d2b9e"
   },
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=23, sampling_strategy='minority')\n",
    "X_resampled, y_resampled = ros.fit_resample(data[['tweet']], data['label'])\n",
    "data_augmentation_noprep = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "data_augmentation_noprep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P1ejVOiF-OTr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1628894201238,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "P1ejVOiF-OTr",
    "outputId": "e315a1ef-2375-43ca-f5d1-113ad0c5d1e3"
   },
   "outputs": [],
   "source": [
    "data_augmentation_noprep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "htgrsK-kELj3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4462040,
     "status": "ok",
     "timestamp": 1628901338697,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "htgrsK-kELj3",
    "outputId": "e1b17281-8073-4ac8-898a-d98be4428ca5"
   },
   "outputs": [],
   "source": [
    "modelos = ['modelo1', 'modelo2', 'modelo3', 'modelo4', 'modelo5', 'modelo6']\n",
    "using_preprocessing = [False, True]\n",
    "using_augmentation = [False, True]\n",
    "use_preprocessing = []\n",
    "modelos_usados = []\n",
    "tempos = []\n",
    "balanceamento_usados = []\n",
    "resultados = pd.DataFrame()\n",
    "scores = []\n",
    "\n",
    "print(\"Treinamento iniciado!\\n\")\n",
    "\n",
    "for modelo in modelos:\n",
    "\n",
    "  for preprocess in using_preprocessing:\n",
    "\n",
    "    for augmentation in using_augmentation:\n",
    "\n",
    "      if augmentation and preprocess:\n",
    "        X_train, X_validation, y_train, y_validation, vocab_size, tokenizer = preprocessing_step(preprocess, test_size, data_augmentation_prep)\n",
    "\n",
    "      elif augmentation and not preprocess:\n",
    "        X_train, X_validation, y_train, y_validation, vocab_size, tokenizer = preprocessing_step(preprocess, test_size, data_augmentation_noprep)\n",
    "\n",
    "      else:\n",
    "        X_train, X_validation, y_train, y_validation, vocab_size, tokenizer = preprocessing_step(preprocess, test_size, data)\n",
    "\n",
    "      print(\"------ \\nTreinando o modelo: {}\\nUsando pré-processamento: {}\\nUsando o balanceamento: {}\\n------\".format(modelo, preprocess, augmentation))\n",
    "\n",
    "      ini = time.time()\n",
    "\n",
    "      if modelo == 'modelo1':\n",
    "        m, score = modelo1(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, preprocess)\n",
    "\n",
    "      elif modelo == 'modelo2':\n",
    "        m, score = modelo2(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, preprocess)\n",
    "        \n",
    "      elif modelo == 'modelo3':\n",
    "        m, score = modelo3(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, preprocess)\n",
    "        \n",
    "      elif modelo == 'modelo4':\n",
    "        m, score = modelo4(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, preprocess)\n",
    "        \n",
    "      elif modelo == 'modelo5':\n",
    "        m, score = modelo5(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, preprocess)\n",
    "        \n",
    "      elif modelo == 'modelo6':\n",
    "        m, score = modelo6(X_train, X_validation, y_train, y_validation, length_size, embedding_dim, vocab_size, preprocess)\n",
    "      \n",
    "      fim = time.time()\n",
    "      tempo = round(fim - ini, 3)\n",
    "      tempos.append(tempo)\n",
    "      print(\"\\nTreinamento finalizado em {} segundos!\".format(tempo))\n",
    "      print()\n",
    "\n",
    "      balanceamento_usados.append(augmentation)\n",
    "      scores.append(round(score, 4))\n",
    "      modelos_usados.append(modelo)\n",
    "      use_preprocessing.append(preprocess)\n",
    "      predict(m, modelo, preprocess, tokenizer, augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9XSXUBIRCg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1628901338713,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "bc9XSXUBIRCg",
    "outputId": "ffeb105c-4b2f-4435-d0ca-19ad12a1a61a"
   },
   "outputs": [],
   "source": [
    "resultados['modelo'] = modelos_usados\n",
    "resultados['tempo (s)'] = tempos\n",
    "resultados['f1_score'] = scores\n",
    "resultados['preprocessamento'] = use_preprocessing\n",
    "resultados['balanceamento'] = balanceamento_usados\n",
    "resultados.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jc8KrT0twfmb",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1628901338714,
     "user": {
      "displayName": "Rafael Greca",
      "photoUrl": "",
      "userId": "13108790060757877730"
     },
     "user_tz": 180
    },
    "id": "Jc8KrT0twfmb"
   },
   "outputs": [],
   "source": [
    "resultados.to_csv('teste_ros.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "laden-parade",
    "r0rq3djpCXxH",
    "59ipy94TLMcs",
    "8r0S-aYwLrSS",
    "XBz-y8sEFvzU",
    "jnO19TNnFzNV",
    "LZUD_tmjqIw3"
   ],
   "name": "cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
